# LangGraph 0.6 Context API êµ¬í˜„ ë©”ë‰´ì–¼

## ğŸ“‹ ëª©ì°¨
1. [ê°œìš”](#ê°œìš”)
2. [ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •](#ì„¤ì¹˜-ë°-í™˜ê²½-ì„¤ì •)
3. [Context API ê¸°ë³¸ êµ¬ì¡°](#context-api-ê¸°ë³¸-êµ¬ì¡°)
4. [ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ](#ë‹¨ê³„ë³„-êµ¬í˜„-ê°€ì´ë“œ)
5. [ê³ ê¸‰ íŒ¨í„´](#ê³ ê¸‰-íŒ¨í„´)
6. [ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](#ë§ˆì´ê·¸ë ˆì´ì…˜-ê°€ì´ë“œ)
7. [íŠ¸ëŸ¬ë¸”ìŠˆíŒ…](#íŠ¸ëŸ¬ë¸”ìŠˆíŒ…)

## ê°œìš”

LangGraph 0.6ì—ì„œ ë„ì…ëœ Context APIëŠ” ëŸ°íƒ€ì„ ì˜ì¡´ì„±ì„ ê´€ë¦¬í•˜ëŠ” ìƒˆë¡œìš´ íŒ¨ëŸ¬ë‹¤ì„ì…ë‹ˆë‹¤. ì´ì „ì˜ `config['configurable']` íŒ¨í„´ì„ ëŒ€ì²´í•˜ì—¬ ë” ëª…í™•í•˜ê³  íƒ€ì… ì•ˆì „í•œ ë°©ì‹ì„ ì œê³µí•©ë‹ˆë‹¤.

### ì£¼ìš” ë³€ê²½ì‚¬í•­
- âŒ **ì´ì „**: `config['configurable']` ì‚¬ìš©
- âœ… **í˜„ì¬**: `Context` ìŠ¤í‚¤ë§ˆì™€ `Runtime` ê°ì²´ ì‚¬ìš©

## ì„¤ì¹˜ ë° í™˜ê²½ ì„¤ì •

### í•„ìˆ˜ íŒ¨í‚¤ì§€ ì„¤ì¹˜
```bash
pip install langgraph>=0.6.0
pip install langchain-core
pip install langchain-openai  # OpenAI ì‚¬ìš© ì‹œ
pip install langchain-anthropic  # Anthropic ì‚¬ìš© ì‹œ
```

### ë²„ì „ í™•ì¸
```python
import langgraph
print(f"LangGraph version: {langgraph.__version__}")
# 0.6.0 ì´ìƒì´ì–´ì•¼ í•¨
```

## Context API ê¸°ë³¸ êµ¬ì¡°

### 1. Context ìŠ¤í‚¤ë§ˆ ì •ì˜

ContextëŠ” ëŸ°íƒ€ì„ì— í•„ìš”í•œ ì •ì  ë°ì´í„°ë¥¼ ì •ì˜í•©ë‹ˆë‹¤. `dataclass`, `TypedDict`, ë˜ëŠ” `Pydantic` ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

#### dataclass ì‚¬ìš© (ê¶Œì¥)
```python
from dataclasses import dataclass
from typing import Optional, Literal

@dataclass
class ContextSchema:
    # í•„ìˆ˜ í•„ë“œ
    user_id: str
    
    # ì„ íƒì  í•„ë“œ (ê¸°ë³¸ê°’ í¬í•¨)
    model_provider: Literal["openai", "anthropic"] = "anthropic"
    model_name: str = "claude-3-5-haiku-latest"
    temperature: float = 0.7
    
    # ì„ íƒì  í•„ë“œ (None í—ˆìš©)
    system_message: Optional[str] = None
    db_connection: Optional[str] = None
    api_key: Optional[str] = None
```

#### TypedDict ì‚¬ìš©
```python
from typing_extensions import TypedDict
from typing import Optional

class ContextSchema(TypedDict, total=False):
    user_id: str  # required=Trueë¡œ ì§€ì • ê°€ëŠ¥
    model_provider: str
    system_message: Optional[str]
```

#### Pydantic ëª¨ë¸ ì‚¬ìš©
```python
from pydantic import BaseModel, Field

class ContextSchema(BaseModel):
    user_id: str = Field(..., description="ì‚¬ìš©ì ê³ ìœ  ID")
    model_provider: str = Field(default="anthropic")
    temperature: float = Field(default=0.7, ge=0, le=2)
```

### 2. State ì •ì˜

StateëŠ” ê·¸ë˜í”„ ì‹¤í–‰ ì¤‘ ë³€ê²½ë˜ëŠ” ë™ì  ë°ì´í„°ë¥¼ ê´€ë¦¬í•©ë‹ˆë‹¤.

```python
from typing import List, Annotated
from typing_extensions import TypedDict
from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages

class State(TypedDict):
    messages: Annotated[List[AnyMessage], add_messages]
    current_step: str
    result: Optional[str]
```

### 3. Runtime ê°ì²´ í™œìš©

Runtime ê°ì²´ëŠ” contextì™€ ê¸°íƒ€ ëŸ°íƒ€ì„ ìœ í‹¸ë¦¬í‹°ì— ì ‘ê·¼í•˜ëŠ” í†µí•© ì¸í„°í˜ì´ìŠ¤ì…ë‹ˆë‹¤.

```python
from langgraph.runtime import Runtime

def my_node(state: State, runtime: Runtime[ContextSchema]) -> State:
    # Context ì ‘ê·¼
    user_id = runtime.context.user_id
    model_provider = runtime.context.model_provider
    
    # Store ì ‘ê·¼ (ë©”ëª¨ë¦¬ ê´€ë¦¬)
    if runtime.store:
        user_data = runtime.store.get(("users", user_id))
    
    # Stream writer ì ‘ê·¼ (ì»¤ìŠ¤í…€ ìŠ¤íŠ¸ë¦¬ë°)
    if runtime.stream_writer:
        runtime.stream_writer({"status": "processing"})
    
    return {"current_step": "completed"}
```

## ë‹¨ê³„ë³„ êµ¬í˜„ ê°€ì´ë“œ

### Step 1: í”„ë¡œì íŠ¸ êµ¬ì¡° ì„¤ì •

```
chatbot_project/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ context.py      # Context ìŠ¤í‚¤ë§ˆ ì •ì˜
â”œâ”€â”€ state.py        # State ì •ì˜
â”œâ”€â”€ nodes.py        # ë…¸ë“œ í•¨ìˆ˜ë“¤
â”œâ”€â”€ tools.py        # ë„êµ¬ ì •ì˜
â”œâ”€â”€ graph.py        # ê·¸ë˜í”„ êµ¬ì„±
â””â”€â”€ main.py         # ì‹¤í–‰ ì—”íŠ¸ë¦¬í¬ì¸íŠ¸
```

### Step 2: Context ìŠ¤í‚¤ë§ˆ ì •ì˜ (context.py)

```python
from dataclasses import dataclass
from typing import Optional, List, Literal

@dataclass
class ChatbotContext:
    """ì±—ë´‡ ëŸ°íƒ€ì„ ì»¨í…ìŠ¤íŠ¸"""
    
    # ì‚¬ìš©ì ì •ë³´
    user_id: str
    user_name: str
    user_role: Literal["admin", "user", "guest"] = "user"
    
    # ëª¨ë¸ ì„¤ì •
    model_provider: Literal["openai", "anthropic", "google"] = "anthropic"
    model_name: str = "claude-3-5-haiku-latest"
    temperature: float = 0.7
    max_tokens: int = 4096
    
    # ì‹œìŠ¤í…œ ì„¤ì •
    system_message: Optional[str] = None
    available_tools: List[str] = None
    
    # ì™¸ë¶€ ë¦¬ì†ŒìŠ¤
    db_connection_string: Optional[str] = None
    api_endpoints: Optional[dict] = None
    
    def __post_init__(self):
        """ì´ˆê¸°í™” í›„ ì²˜ë¦¬"""
        if self.available_tools is None:
            self.available_tools = []
        if self.api_endpoints is None:
            self.api_endpoints = {}
```

### Step 3: State ì •ì˜ (state.py)

```python
from typing import List, Optional, Dict, Any, Annotated
from typing_extensions import TypedDict
from langchain_core.messages import AnyMessage
from langgraph.graph import add_messages

class ChatbotState(TypedDict):
    """ì±—ë´‡ ìƒíƒœ ê´€ë¦¬"""
    
    # ëŒ€í™” ê´€ë ¨
    messages: Annotated[List[AnyMessage], add_messages]
    
    # ì›Œí¬í”Œë¡œìš° ê´€ë ¨
    current_node: str
    next_action: Optional[str]
    
    # ë„êµ¬ ì‹¤í–‰ ê´€ë ¨
    tool_calls: List[Dict[str, Any]]
    tool_results: List[Dict[str, Any]]
    
    # ê²°ê³¼ ê´€ë ¨
    final_response: Optional[str]
    metadata: Dict[str, Any]
```

### Step 4: ë…¸ë“œ êµ¬í˜„ (nodes.py)

```python
from typing import Dict, Any
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.chat_models import init_chat_model
from langgraph.runtime import Runtime
from .context import ChatbotContext
from .state import ChatbotState

# ëª¨ë¸ ì´ˆê¸°í™” ë§µ
MODELS = {
    "openai": {
        "gpt-4": "openai:gpt-4",
        "gpt-4-turbo": "openai:gpt-4-turbo-preview",
        "gpt-3.5-turbo": "openai:gpt-3.5-turbo"
    },
    "anthropic": {
        "claude-3-opus": "anthropic:claude-3-opus-latest",
        "claude-3-5-sonnet": "anthropic:claude-3-5-sonnet-latest",
        "claude-3-5-haiku": "anthropic:claude-3-5-haiku-latest"
    }
}

def preprocess_node(
    state: ChatbotState, 
    runtime: Runtime[ChatbotContext]
) -> Dict[str, Any]:
    """ë©”ì‹œì§€ ì „ì²˜ë¦¬ ë…¸ë“œ"""
    
    # Contextì—ì„œ ì‚¬ìš©ì ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    user_name = runtime.context.user_name
    user_role = runtime.context.user_role
    
    # ì‹œìŠ¤í…œ ë©”ì‹œì§€ ì¶”ê°€
    messages = state["messages"].copy()
    
    if runtime.context.system_message:
        system_msg = runtime.context.system_message.format(
            user_name=user_name,
            user_role=user_role
        )
        messages.insert(0, SystemMessage(content=system_msg))
    
    return {
        "messages": messages,
        "current_node": "preprocess",
        "metadata": {
            "user_id": runtime.context.user_id,
            "timestamp": datetime.now().isoformat()
        }
    }

def llm_node(
    state: ChatbotState,
    runtime: Runtime[ChatbotContext]
) -> Dict[str, Any]:
    """LLM í˜¸ì¶œ ë…¸ë“œ"""
    
    # Contextì—ì„œ ëª¨ë¸ ì„¤ì • ê°€ì ¸ì˜¤ê¸°
    provider = runtime.context.model_provider
    model_name = runtime.context.model_name
    
    # ëª¨ë¸ ì´ˆê¸°í™”
    model_string = f"{provider}:{model_name}"
    model = init_chat_model(
        model_string,
        temperature=runtime.context.temperature,
        max_tokens=runtime.context.max_tokens
    )
    
    # ë„êµ¬ ë°”ì¸ë”© (í•„ìš”í•œ ê²½ìš°)
    if runtime.context.available_tools:
        tools = load_tools(runtime.context.available_tools)
        model = model.bind_tools(tools)
    
    # ëª¨ë¸ í˜¸ì¶œ
    response = model.invoke(state["messages"])
    
    # ìŠ¤íŠ¸ë¦¼ ë¼ì´í„° ì‚¬ìš© (ì˜µì…˜)
    if runtime.stream_writer:
        runtime.stream_writer({
            "type": "llm_response",
            "content": response.content
        })
    
    return {
        "messages": [response],
        "current_node": "llm",
        "final_response": response.content
    }

def memory_node(
    state: ChatbotState,
    runtime: Runtime[ChatbotContext]
) -> Dict[str, Any]:
    """ë©”ëª¨ë¦¬ ì €ì¥ ë…¸ë“œ"""
    
    if not runtime.store:
        return {"current_node": "memory"}
    
    user_id = runtime.context.user_id
    
    # ëŒ€í™” ë‚´ì—­ ì €ì¥
    conversation = {
        "messages": [msg.dict() for msg in state["messages"]],
        "timestamp": datetime.now().isoformat(),
        "metadata": state.get("metadata", {})
    }
    
    # Storeì— ì €ì¥
    runtime.store.put(
        ("conversations", user_id),
        str(uuid.uuid4()),
        conversation
    )
    
    # ì‚¬ìš©ì í†µê³„ ì—…ë°ì´íŠ¸
    stats = runtime.store.get(("stats", user_id)) or {"count": 0}
    stats["count"] += 1
    stats["last_interaction"] = datetime.now().isoformat()
    runtime.store.put(("stats",), user_id, stats)
    
    return {"current_node": "memory"}
```

### Step 5: ë„êµ¬ êµ¬í˜„ (tools.py)

```python
from typing import Optional
from langchain_core.tools import tool
from langgraph.runtime import get_runtime
from .context import ChatbotContext

@tool
def get_user_preferences() -> dict:
    """ì‚¬ìš©ì ì„ í˜¸ë„ ì¡°íšŒ"""
    runtime = get_runtime(ChatbotContext)
    user_id = runtime.context.user_id
    
    # Contextì—ì„œ DB ì—°ê²° ì •ë³´ ì‚¬ìš©
    if runtime.context.db_connection_string:
        # DBì—ì„œ ì¡°íšŒ ë¡œì§
        preferences = fetch_from_db(
            runtime.context.db_connection_string,
            user_id
        )
        return preferences
    
    return {"preferences": "default"}

@tool
def call_external_api(endpoint: str, params: dict) -> dict:
    """ì™¸ë¶€ API í˜¸ì¶œ"""
    runtime = get_runtime(ChatbotContext)
    
    # Contextì—ì„œ API ì—”ë“œí¬ì¸íŠ¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
    api_config = runtime.context.api_endpoints.get(endpoint)
    if not api_config:
        return {"error": f"Unknown endpoint: {endpoint}"}
    
    # API í˜¸ì¶œ ë¡œì§
    response = make_api_call(api_config, params)
    return response

def load_tools(tool_names: List[str]):
    """ë„êµ¬ ë™ì  ë¡œë“œ"""
    available_tools = {
        "get_user_preferences": get_user_preferences,
        "call_external_api": call_external_api,
        # ì¶”ê°€ ë„êµ¬ë“¤...
    }
    
    return [available_tools[name] for name in tool_names if name in available_tools]
```

### Step 6: ê·¸ë˜í”„ êµ¬ì„± (graph.py)

```python
from langgraph.graph import StateGraph, START, END
from langgraph.checkpoint.memory import MemorySaver
from langgraph.store.memory import InMemoryStore
from .context import ChatbotContext
from .state import ChatbotState
from .nodes import preprocess_node, llm_node, memory_node

def create_chatbot_graph():
    """ì±—ë´‡ ê·¸ë˜í”„ ìƒì„±"""
    
    # ê·¸ë˜í”„ ë¹Œë” ìƒì„±
    builder = StateGraph(
        state_schema=ChatbotState,
        context_schema=ChatbotContext  # Context ìŠ¤í‚¤ë§ˆ ë“±ë¡
    )
    
    # ë…¸ë“œ ì¶”ê°€
    builder.add_node("preprocess", preprocess_node)
    builder.add_node("llm", llm_node)
    builder.add_node("memory", memory_node)
    
    # ì—£ì§€ ì •ì˜
    builder.add_edge(START, "preprocess")
    builder.add_edge("preprocess", "llm")
    builder.add_edge("llm", "memory")
    builder.add_edge("memory", END)
    
    # ì¡°ê±´ë¶€ ì—£ì§€ (ì˜µì…˜)
    def should_continue(state: ChatbotState) -> str:
        if state.get("tool_calls"):
            return "tools"
        return "end"
    
    builder.add_conditional_edges(
        "llm",
        should_continue,
        {
            "tools": "tool_execution",
            "end": "memory"
        }
    )
    
    # ì²´í¬í¬ì¸í„°ì™€ ìŠ¤í† ì–´ ì„¤ì •
    checkpointer = MemorySaver()
    store = InMemoryStore()
    
    # ê·¸ë˜í”„ ì»´íŒŒì¼
    graph = builder.compile(
        checkpointer=checkpointer,
        store=store
    )
    
    return graph
```

### Step 7: ì‹¤í–‰ (main.py)

```python
import asyncio
from typing import Optional
from .graph import create_chatbot_graph
from .context import ChatbotContext

class ChatbotApp:
    def __init__(self):
        self.graph = create_chatbot_graph()
    
    def create_context(
        self,
        user_id: str,
        user_name: str,
        **kwargs
    ) -> ChatbotContext:
        """ì‚¬ìš©ìë³„ Context ìƒì„±"""
        return ChatbotContext(
            user_id=user_id,
            user_name=user_name,
            **kwargs
        )
    
    def chat(
        self,
        message: str,
        user_id: str,
        user_name: str,
        thread_id: Optional[str] = None,
        **context_kwargs
    ):
        """ë™ê¸° ì±„íŒ…"""
        # Context ìƒì„±
        context = self.create_context(
            user_id=user_id,
            user_name=user_name,
            **context_kwargs
        )
        
        # ì…ë ¥ ì¤€ë¹„
        input_state = {
            "messages": [{"role": "user", "content": message}]
        }
        
        # ì„¤ì • ì¤€ë¹„
        config = {
            "configurable": {
                "thread_id": thread_id or f"thread_{user_id}"
            }
        }
        
        # ê·¸ë˜í”„ ì‹¤í–‰
        result = self.graph.invoke(
            input_state,
            context=context,  # Context ì „ë‹¬
            config=config
        )
        
        return result["final_response"]
    
    async def achat(
        self,
        message: str,
        user_id: str,
        user_name: str,
        thread_id: Optional[str] = None,
        **context_kwargs
    ):
        """ë¹„ë™ê¸° ì±„íŒ…"""
        context = self.create_context(
            user_id=user_id,
            user_name=user_name,
            **context_kwargs
        )
        
        input_state = {
            "messages": [{"role": "user", "content": message}]
        }
        
        config = {
            "configurable": {
                "thread_id": thread_id or f"thread_{user_id}"
            }
        }
        
        # ë¹„ë™ê¸° ì‹¤í–‰
        result = await self.graph.ainvoke(
            input_state,
            context=context,
            config=config
        )
        
        return result["final_response"]
    
    def stream_chat(
        self,
        message: str,
        user_id: str,
        user_name: str,
        **context_kwargs
    ):
        """ìŠ¤íŠ¸ë¦¬ë° ì±„íŒ…"""
        context = self.create_context(
            user_id=user_id,
            user_name=user_name,
            **context_kwargs
        )
        
        input_state = {
            "messages": [{"role": "user", "content": message}]
        }
        
        # ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        for chunk in self.graph.stream(
            input_state,
            context=context,
            stream_mode="values"
        ):
            yield chunk

# ì‚¬ìš© ì˜ˆì œ
if __name__ == "__main__":
    app = ChatbotApp()
    
    # ê¸°ë³¸ ì‚¬ìš©
    response = app.chat(
        message="ì•ˆë…•í•˜ì„¸ìš”!",
        user_id="user123",
        user_name="í™ê¸¸ë™"
    )
    print(response)
    
    # ì»¤ìŠ¤í…€ Context ì„¤ì •
    response = app.chat(
        message="ë‚ ì”¨ ì•Œë ¤ì¤˜",
        user_id="user123",
        user_name="í™ê¸¸ë™",
        model_provider="openai",
        model_name="gpt-4",
        temperature=0.5,
        available_tools=["get_weather", "get_news"],
        system_message="ë‹¹ì‹ ì€ ì¹œì ˆí•œ AI ë¹„ì„œì…ë‹ˆë‹¤."
    )
    print(response)
    
    # ë¹„ë™ê¸° ì‚¬ìš©
    async def async_example():
        response = await app.achat(
            message="íŒŒì´ì¬ ì½”ë“œ ì‘ì„±í•´ì¤˜",
            user_id="user456",
            user_name="ê¹€ì² ìˆ˜",
            model_provider="anthropic",
            model_name="claude-3-opus"
        )
        print(response)
    
    asyncio.run(async_example())
```

## ê³ ê¸‰ íŒ¨í„´

### 1. ë™ì  ëª¨ë¸ ì„ íƒ

```python
def dynamic_model_selection(
    state: State,
    runtime: Runtime[ContextSchema]
) -> Dict[str, Any]:
    """ìš”ì²­ ë³µì¡ë„ì— ë”°ë¥¸ ë™ì  ëª¨ë¸ ì„ íƒ"""
    
    message_length = len(state["messages"][-1].content)
    
    # ë³µì¡ë„ì— ë”°ë¼ ëª¨ë¸ ì„ íƒ
    if message_length < 100:
        model_name = "gpt-3.5-turbo"  # ê°„ë‹¨í•œ ì§ˆë¬¸
    elif message_length < 500:
        model_name = "gpt-4"  # ì¤‘ê°„ ë³µì¡ë„
    else:
        model_name = "claude-3-opus"  # ë³µì¡í•œ ìš”ì²­
    
    # Context ì˜¤ë²„ë¼ì´ë“œ (ì§ì ‘ ìˆ˜ì •ì€ ë¶ˆê°€, ìƒˆ ëª¨ë¸ë¡œ ì²˜ë¦¬)
    model = init_chat_model(model_name)
    response = model.invoke(state["messages"])
    
    return {"messages": [response]}
```

### 2. ë‹¤ì¤‘ Context íŒ¨í„´

```python
@dataclass
class BaseContext:
    """ê¸°ë³¸ Context"""
    user_id: str
    timestamp: str

@dataclass
class ModelContext:
    """ëª¨ë¸ ê´€ë ¨ Context"""
    provider: str
    model_name: str
    temperature: float

@dataclass
class FullContext(BaseContext, ModelContext):
    """í†µí•© Context"""
    pass

# ì‚¬ìš©
def node_with_full_context(
    state: State,
    runtime: Runtime[FullContext]
) -> Dict[str, Any]:
    # ëª¨ë“  Context í•„ë“œ ì ‘ê·¼ ê°€ëŠ¥
    user_id = runtime.context.user_id
    model_name = runtime.context.model_name
    # ...
```

### 3. Context ê²€ì¦

```python
from pydantic import BaseModel, validator

class ValidatedContext(BaseModel):
    """ê²€ì¦ì´ í¬í•¨ëœ Context"""
    
    user_id: str
    api_key: str
    rate_limit: int
    
    @validator('user_id')
    def validate_user_id(cls, v):
        if not v or len(v) < 3:
            raise ValueError("Invalid user_id")
        return v
    
    @validator('rate_limit')
    def validate_rate_limit(cls, v):
        if v < 0 or v > 1000:
            raise ValueError("Rate limit must be between 0 and 1000")
        return v
```

### 4. Context íŒ©í† ë¦¬ íŒ¨í„´

```python
class ContextFactory:
    """Context ìƒì„± íŒ©í† ë¦¬"""
    
    @staticmethod
    def create_for_admin(user_id: str) -> ChatbotContext:
        return ChatbotContext(
            user_id=user_id,
            user_name="Admin",
            user_role="admin",
            model_provider="openai",
            model_name="gpt-4",
            temperature=0.3,
            available_tools=["admin_tools"]
        )
    
    @staticmethod
    def create_for_guest() -> ChatbotContext:
        return ChatbotContext(
            user_id="guest_" + str(uuid.uuid4()),
            user_name="Guest",
            user_role="guest",
            model_provider="openai",
            model_name="gpt-3.5-turbo",
            temperature=0.7,
            max_tokens=1000
        )
```

## ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ

### ì´ì „ ì½”ë“œ (config ì‚¬ìš©)

```python
# ì´ì „ ë°©ì‹
def old_node(state: State, config: RunnableConfig):
    user_id = config.get("configurable", {}).get("user_id")
    model = config.get("configurable", {}).get("model", "gpt-3.5-turbo")
    # ...

# ê·¸ë˜í”„ ìƒì„±
builder = StateGraph(State, config_schema=ConfigSchema)

# ì‹¤í–‰
graph.invoke(
    input_state,
    config={"configurable": {"user_id": "123", "model": "gpt-4"}}
)
```

### ìƒˆë¡œìš´ ì½”ë“œ (Context ì‚¬ìš©)

```python
# ìƒˆë¡œìš´ ë°©ì‹
@dataclass
class ContextSchema:
    user_id: str
    model: str = "gpt-3.5-turbo"

def new_node(state: State, runtime: Runtime[ContextSchema]):
    user_id = runtime.context.user_id
    model = runtime.context.model
    # ...

# ê·¸ë˜í”„ ìƒì„±
builder = StateGraph(State, context_schema=ContextSchema)

# ì‹¤í–‰
graph.invoke(
    input_state,
    context={"user_id": "123", "model": "gpt-4"}
)
```

### ë§ˆì´ê·¸ë ˆì´ì…˜ ì²´í¬ë¦¬ìŠ¤íŠ¸

- [ ] `config_schema`ë¥¼ `context_schema`ë¡œ ë³€ê²½
- [ ] ë…¸ë“œ ì‹œê·¸ë‹ˆì²˜ë¥¼ `(state, config)`ì—ì„œ `(state, runtime)`ìœ¼ë¡œ ë³€ê²½
- [ ] `config["configurable"]` ì ‘ê·¼ì„ `runtime.context`ë¡œ ë³€ê²½
- [ ] `graph.invoke()` í˜¸ì¶œ ì‹œ `config` ëŒ€ì‹  `context` ì‚¬ìš©
- [ ] ë„êµ¬ì—ì„œ `get_config()`ë¥¼ `get_runtime()`ìœ¼ë¡œ ë³€ê²½

## íŠ¸ëŸ¬ë¸”ìŠˆíŒ…

### 1. TypeError: Context ì´ˆê¸°í™” ì˜¤ë¥˜

**ë¬¸ì œ**: LangGraph API ì„œë²„ ì‚¬ìš© ì‹œ HTTP í—¤ë”ê°€ Contextì— ë³‘í•©ë˜ì–´ ì˜¤ë¥˜ ë°œìƒ

**í•´ê²°**:
```python
@dataclass
class ContextSchema:
    user_id: str
    # ê¸°íƒ€ í•„ë“œ...
    
    def __init__(self, user_id: str, **kwargs):
        """ì¶”ê°€ kwargs ë¬´ì‹œ"""
        self.user_id = user_id
        # ëª…ì‹œì ìœ¼ë¡œ í•„ìš”í•œ í•„ë“œë§Œ ì„¤ì •
```

### 2. Contextê°€ ì„œë¸Œê·¸ë˜í”„ë¡œ ì „ë‹¬ë˜ì§€ ì•ŠìŒ

**ë¬¸ì œ**: ë©”ì¸ ê·¸ë˜í”„ì˜ Contextê°€ ì„œë¸Œê·¸ë˜í”„ë¡œ ìë™ ì „ë‹¬ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```python
def parent_node(state: State, runtime: Runtime[ContextSchema]):
    # ì„œë¸Œê·¸ë˜í”„ í˜¸ì¶œ ì‹œ ëª…ì‹œì ìœ¼ë¡œ Context ì „ë‹¬
    subgraph_result = subgraph.invoke(
        state,
        context=runtime.context  # Context ëª…ì‹œì  ì „ë‹¬
    )
    return subgraph_result
```

### 3. Runtimeì´ Noneìœ¼ë¡œ ë‚˜íƒ€ë‚¨

**ë¬¸ì œ**: ìŠ¤íŠ¸ë¦¬ë° ì—”ë“œí¬ì¸íŠ¸ì—ì„œ runtime.contextê°€ null

**í•´ê²°**:
```python
# ìŠ¤íŠ¸ë¦¬ë° ì‹œ Context í™•ì¸
def node(state: State, runtime: Runtime[ContextSchema]):
    if runtime and runtime.context:
        user_id = runtime.context.user_id
    else:
        # ê¸°ë³¸ê°’ ì‚¬ìš©
        user_id = "default_user"
```

### 4. ë™ì  ë„êµ¬ ë°”ì¸ë”© ì‹¤íŒ¨

**ë¬¸ì œ**: Contextì˜ available_toolsê°€ ì œëŒ€ë¡œ ë°”ì¸ë”©ë˜ì§€ ì•ŠìŒ

**í•´ê²°**:
```python
def llm_node(state: State, runtime: Runtime[ContextSchema]):
    # ë„êµ¬ë¥¼ ë¨¼ì € ë¡œë“œí•˜ê³  ê²€ì¦
    if runtime.context.available_tools:
        tools = []
        for tool_name in runtime.context.available_tools:
            tool = load_tool(tool_name)
            if tool:
                tools.append(tool)
        
        if tools:
            model = model.bind_tools(tools)
```

## ë² ìŠ¤íŠ¸ í”„ë™í‹°ìŠ¤

### 1. Context ì„¤ê³„ ì›ì¹™
- ë¶ˆë³€ ë°ì´í„°ë§Œ Contextì— í¬í•¨
- ìì£¼ ë³€ê²½ë˜ëŠ” ë°ì´í„°ëŠ” State ì‚¬ìš©
- ë¯¼ê°í•œ ì •ë³´ëŠ” ì•”í˜¸í™”í•˜ê±°ë‚˜ ë³„ë„ ê´€ë¦¬

### 2. íƒ€ì… ì•ˆì „ì„±
- í•­ìƒ íƒ€ì… íŒíŠ¸ ì‚¬ìš©
- Runtime[ContextSchema] í˜•íƒœë¡œ ëª…ì‹œ
- Optional í•„ë“œëŠ” ëª…í™•íˆ í‘œì‹œ

### 3. ì„±ëŠ¥ ìµœì í™”
- Context ê°ì²´ë¥¼ ê°€ë³ê²Œ ìœ ì§€
- ëŒ€ìš©ëŸ‰ ë°ì´í„°ëŠ” ì°¸ì¡°ë§Œ ì €ì¥
- í•„ìš”í•œ ê²½ìš° lazy loading íŒ¨í„´ ì‚¬ìš©

### 4. í…ŒìŠ¤íŠ¸
```python
import pytest
from unittest.mock import Mock

def test_node_with_context():
    # Mock Runtime ìƒì„±
    mock_runtime = Mock()
    mock_runtime.context = ContextSchema(
        user_id="test_user",
        model_provider="openai"
    )
    
    # ë…¸ë“œ í…ŒìŠ¤íŠ¸
    result = my_node(test_state, mock_runtime)
    assert result["status"] == "success"
```

## ì°¸ê³  ìë£Œ

- [LangGraph ê³µì‹ ë¬¸ì„œ](https://langchain-ai.github.io/langgraph/)
- [Context API ê°€ì´ë“œ](https://langchain-ai.github.io/langgraph/agents/context/)
- [Runtime API ë ˆí¼ëŸ°ìŠ¤](https://langchain-ai.github.io/langgraph/reference/runtime/)
- [ë§ˆì´ê·¸ë ˆì´ì…˜ ê°€ì´ë“œ](https://langchain-ai.github.io/langgraph/migration/)

## ìš”ì•½

LangGraph 0.6ì˜ Context APIëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì¥ì ì„ ì œê³µí•©ë‹ˆë‹¤:

1. **íƒ€ì… ì•ˆì „ì„±**: ëª…í™•í•œ ìŠ¤í‚¤ë§ˆ ì •ì˜ì™€ íƒ€ì… ì²´í‚¹
2. **ì˜ì¡´ì„± ì£¼ì…**: ê¹”ë”í•œ ì˜ì¡´ì„± ê´€ë¦¬
3. **í…ŒìŠ¤íŠ¸ ìš©ì´ì„±**: Mock ê°ì²´ë¥¼ í†µí•œ ì‰¬ìš´ í…ŒìŠ¤íŠ¸
4. **ìœ ì§€ë³´ìˆ˜ì„±**: ëª…í™•í•œ ì½”ë“œ êµ¬ì¡°ì™€ ë¬¸ì„œí™”
5. **í™•ì¥ì„±**: ë³µì¡í•œ ì—”í„°í”„ë¼ì´ì¦ˆ ì• í”Œë¦¬ì¼€ì´ì…˜ ì§€ì›

ì´ ë©”ë‰´ì–¼ì„ ë”°ë¼ êµ¬í˜„í•˜ë©´ í”„ë¡œë•ì…˜ ë ˆë²¨ì˜ ê²¬ê³ í•œ ì±—ë´‡ì„ êµ¬ì¶•í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.
