"""
ë¬¸ì„œ ì²­í‚¹ ëª¨ë“ˆ
- ì˜ë¯¸ ê¸°ë°˜ ì²­í‚¹
- í‘œ êµ¬ì¡° ë³´ì¡´ ì²­í‚¹
- ì˜¤ë²„ë© ì²˜ë¦¬
"""
import re
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
import tiktoken
import logging

logger = logging.getLogger(__name__)

@dataclass
class Chunk:
    """ì²­í¬ ë°ì´í„° í´ë˜ìŠ¤"""
    content: str
    chunk_id: str
    doc_id: str
    chunk_index: int
    chunk_type: str  # table, text, mixed
    metadata: Dict[str, Any]
    token_count: int
    char_count: int
    overlap_with_previous: bool = False
    overlap_with_next: bool = False

class DocumentChunker:
    """ë¬¸ì„œ ì²­í‚¹ í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        chunk_size: int = 500,
        chunk_overlap: int = 50,
        min_chunk_size: int = 100,
        tokenizer_model: str = "cl100k_base"
    ):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.min_chunk_size = min_chunk_size
        
        # í† í¬ë‚˜ì´ì € ì´ˆê¸°í™”
        try:
            self.tokenizer = tiktoken.get_encoding(tokenizer_model)
        except:
            logger.warning(f"Failed to load {tokenizer_model}, using default")
            self.tokenizer = tiktoken.get_encoding("cl100k_base")
        
        # ì²­í‚¹ êµ¬ë¶„ì (ìš°ì„ ìˆœìœ„ ìˆœ)
        self.separators = [
            "\n\n[TABLE]",  # í‘œ êµ¬ë¶„ì
            "\n[/TABLE]\n",  # í‘œ ì¢…ë£Œ
            "\n\n",  # ì´ì¤‘ ê°œí–‰
            "\n",    # ë‹¨ì¼ ê°œí–‰
            ". ",    # ë¬¸ì¥ ë
            ", ",    # ì‰¼í‘œ
            " ",     # ê³µë°±
        ]
    
    def chunk_document(
        self, 
        processed_data: Dict[str, Any], 
        doc_id: str
    ) -> List[Chunk]:
        """ë¬¸ì„œ ì „ì²´ë¥¼ ì²­í‚¹"""
        chunks = []
        
        # êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ê¸°ë°˜ ì²­í‚¹
        if 'structured_content' in processed_data:
            chunks = self._chunk_structured_content(
                processed_data['structured_content'], 
                doc_id
            )
        else:
            # ì¼ë°˜ í…ìŠ¤íŠ¸ ì²­í‚¹
            all_text = self._extract_all_text(processed_data)
            chunks = self._chunk_text(all_text, doc_id)
        
        # ì˜¤ë²„ë© ì²˜ë¦¬
        chunks = self._add_overlaps(chunks)
        
        return chunks
    
    def _chunk_structured_content(
        self, 
        structured_content: List[Dict], 
        doc_id: str
    ) -> List[Chunk]:
        """êµ¬ì¡°í™”ëœ ì½˜í…ì¸  ì²­í‚¹"""
        chunks = []
        chunk_index = 0
        
        for item in structured_content:
            if item['type'] == 'table':
                # í‘œëŠ” í•˜ë‚˜ì˜ ì²­í¬ë¡œ ì²˜ë¦¬ (í¬ê¸°ê°€ í¬ë©´ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• )
                table_chunks = self._chunk_table(
                    item['content'], 
                    doc_id, 
                    chunk_index
                )
                chunks.extend(table_chunks)
                chunk_index += len(table_chunks)
                
            elif item['type'] == 'text':
                # í…ìŠ¤íŠ¸ ì²­í‚¹
                text = item['content'].get('cleaned', '')
                if text:
                    text_chunks = self._chunk_text_content(
                        text, 
                        doc_id, 
                        chunk_index,
                        metadata=item['content']
                    )
                    chunks.extend(text_chunks)
                    chunk_index += len(text_chunks)
        
        return chunks
    
    def _chunk_table(
        self, 
        table_data: Dict, 
        doc_id: str, 
        start_index: int
    ) -> List[Chunk]:
        """í‘œ ë°ì´í„° ì²­í‚¹"""
        chunks = []
        
        # ì „ì²´ í‘œ í…ìŠ¤íŠ¸
        table_text = table_data.get('formatted_text', '')
        token_count = self._count_tokens(table_text)
        
        # í‘œê°€ ì‘ìœ¼ë©´ í•˜ë‚˜ì˜ ì²­í¬ë¡œ
        if token_count <= self.chunk_size:
            chunk = Chunk(
                content=table_text,
                chunk_id=f"{doc_id}_chunk_{start_index}",
                doc_id=doc_id,
                chunk_index=start_index,
                chunk_type='table',
                metadata={
                    'headers': table_data.get('headers', []),
                    'row_count': len(table_data.get('rows', [])),
                    'is_complete_table': True
                },
                token_count=token_count,
                char_count=len(table_text)
            )
            chunks.append(chunk)
        else:
            # í‘œê°€ í¬ë©´ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• 
            chunks = self._split_large_table(
                table_data, 
                doc_id, 
                start_index
            )
        
        return chunks
    
    def _split_large_table(
        self, 
        table_data: Dict, 
        doc_id: str, 
        start_index: int
    ) -> List[Chunk]:
        """í° í‘œë¥¼ í–‰ ë‹¨ìœ„ë¡œ ë¶„í• """
        chunks = []
        headers = table_data.get('headers', [])
        rows = table_data.get('rows', [])
        
        current_chunk_rows = []
        current_tokens = 0
        chunk_index = start_index
        
        # í—¤ë” í…ìŠ¤íŠ¸ (ëª¨ë“  ì²­í¬ì— í¬í•¨)
        header_text = f"[í‘œ í—¤ë”: {', '.join(headers)}]\n"
        header_tokens = self._count_tokens(header_text)
        
        for row in rows:
            row_text = self._format_table_row(headers, row)
            row_tokens = self._count_tokens(row_text)
            
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ê°€ëŠ¥í•œì§€ í™•ì¸
            if current_tokens + row_tokens + header_tokens <= self.chunk_size:
                current_chunk_rows.append(row_text)
                current_tokens += row_tokens
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk_rows:
                    chunk_content = header_text + '\n'.join(current_chunk_rows)
                    chunk = Chunk(
                        content=chunk_content,
                        chunk_id=f"{doc_id}_chunk_{chunk_index}",
                        doc_id=doc_id,
                        chunk_index=chunk_index,
                        chunk_type='table_partial',
                        metadata={
                            'headers': headers,
                            'row_count': len(current_chunk_rows),
                            'is_complete_table': False,
                            'part_of_table': True
                        },
                        token_count=current_tokens + header_tokens,
                        char_count=len(chunk_content)
                    )
                    chunks.append(chunk)
                    chunk_index += 1
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk_rows = [row_text]
                current_tokens = row_tokens
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk_rows:
            chunk_content = header_text + '\n'.join(current_chunk_rows)
            chunk = Chunk(
                content=chunk_content,
                chunk_id=f"{doc_id}_chunk_{chunk_index}",
                doc_id=doc_id,
                chunk_index=chunk_index,
                chunk_type='table_partial',
                metadata={
                    'headers': headers,
                    'row_count': len(current_chunk_rows),
                    'is_complete_table': False,
                    'part_of_table': True
                },
                token_count=current_tokens + header_tokens,
                char_count=len(chunk_content)
            )
            chunks.append(chunk)
        
        return chunks
    
    def _format_table_row(self, headers: List[str], row: List[str]) -> str:
        """í‘œ í–‰ì„ í…ìŠ¤íŠ¸ë¡œ í¬ë§·íŒ…"""
        formatted = []
        for header, cell in zip(headers, row):
            if header and cell:
                formatted.append(f"{header}: {cell}")
        return ' | '.join(formatted)
    
    def _chunk_text_content(
        self, 
        text: str, 
        doc_id: str, 
        start_index: int,
        metadata: Dict = None
    ) -> List[Chunk]:
        """í…ìŠ¤íŠ¸ ì½˜í…ì¸  ì²­í‚¹"""
        chunks = []
        
        # ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        segments = self._split_by_separators(text)
        
        current_chunk = []
        current_tokens = 0
        chunk_index = start_index
        
        for segment in segments:
            segment_tokens = self._count_tokens(segment)
            
            # ë‹¨ì¼ ì„¸ê·¸ë¨¼íŠ¸ê°€ ë„ˆë¬´ í° ê²½ìš°
            if segment_tokens > self.chunk_size:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk_content = ''.join(current_chunk)
                    chunks.append(self._create_chunk(
                        chunk_content, doc_id, chunk_index, 'text', metadata
                    ))
                    chunk_index += 1
                    current_chunk = []
                    current_tokens = 0
                
                # í° ì„¸ê·¸ë¨¼íŠ¸ë¥¼ ê°•ì œ ë¶„í• 
                sub_chunks = self._force_split_segment(
                    segment, doc_id, chunk_index, metadata
                )
                chunks.extend(sub_chunks)
                chunk_index += len(sub_chunks)
                
            # í˜„ì¬ ì²­í¬ì— ì¶”ê°€ ê°€ëŠ¥
            elif current_tokens + segment_tokens <= self.chunk_size:
                current_chunk.append(segment)
                current_tokens += segment_tokens
                
            # ìƒˆ ì²­í¬ í•„ìš”
            else:
                # í˜„ì¬ ì²­í¬ ì €ì¥
                if current_chunk:
                    chunk_content = ''.join(current_chunk)
                    chunks.append(self._create_chunk(
                        chunk_content, doc_id, chunk_index, 'text', metadata
                    ))
                    chunk_index += 1
                
                # ìƒˆ ì²­í¬ ì‹œì‘
                current_chunk = [segment]
                current_tokens = segment_tokens
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì €ì¥
        if current_chunk:
            chunk_content = ''.join(current_chunk)
            # ìµœì†Œ í¬ê¸° í™•ì¸
            if self._count_tokens(chunk_content) >= self.min_chunk_size:
                chunks.append(self._create_chunk(
                    chunk_content, doc_id, chunk_index, 'text', metadata
                ))
            elif chunks:
                # ë„ˆë¬´ ì‘ìœ¼ë©´ ì´ì „ ì²­í¬ì™€ ë³‘í•©
                chunks[-1].content += '\n' + chunk_content
                chunks[-1].token_count = self._count_tokens(chunks[-1].content)
                chunks[-1].char_count = len(chunks[-1].content)
        
        return chunks
    
    def _split_by_separators(self, text: str) -> List[str]:
        """êµ¬ë¶„ìë¥¼ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ë¶„í• """
        segments = [text]
        
        for separator in self.separators:
            new_segments = []
            for segment in segments:
                if separator in segment:
                    parts = segment.split(separator)
                    for i, part in enumerate(parts):
                        if part:
                            # êµ¬ë¶„ì ë³µì› (ë§ˆì§€ë§‰ ë¶€ë¶„ ì œì™¸)
                            if i < len(parts) - 1:
                                new_segments.append(part + separator)
                            else:
                                new_segments.append(part)
                else:
                    new_segments.append(segment)
            segments = new_segments
        
        return [s for s in segments if s.strip()]
    
    def _force_split_segment(
        self, 
        segment: str, 
        doc_id: str, 
        start_index: int,
        metadata: Dict = None
    ) -> List[Chunk]:
        """í° ì„¸ê·¸ë¨¼íŠ¸ ê°•ì œ ë¶„í• """
        chunks = []
        words = segment.split()
        
        current_chunk = []
        current_tokens = 0
        chunk_index = start_index
        
        for word in words:
            word_tokens = self._count_tokens(word + ' ')
            
            if current_tokens + word_tokens <= self.chunk_size:
                current_chunk.append(word)
                current_tokens += word_tokens
            else:
                if current_chunk:
                    chunk_content = ' '.join(current_chunk)
                    chunks.append(self._create_chunk(
                        chunk_content, doc_id, chunk_index, 'text_forced_split', metadata
                    ))
                    chunk_index += 1
                
                current_chunk = [word]
                current_tokens = word_tokens
        
        if current_chunk:
            chunk_content = ' '.join(current_chunk)
            chunks.append(self._create_chunk(
                chunk_content, doc_id, chunk_index, 'text_forced_split', metadata
            ))
        
        return chunks
    
    def _add_overlaps(self, chunks: List[Chunk]) -> List[Chunk]:
        """ì²­í¬ ê°„ ì˜¤ë²„ë© ì¶”ê°€"""
        if not chunks or self.chunk_overlap <= 0:
            return chunks
        
        for i in range(len(chunks) - 1):
            current_chunk = chunks[i]
            next_chunk = chunks[i + 1]
            
            # í˜„ì¬ ì²­í¬ì˜ ë ë¶€ë¶„ ì¶”ì¶œ
            current_words = current_chunk.content.split()
            overlap_words = current_words[-self.chunk_overlap:] if len(current_words) > self.chunk_overlap else current_words
            
            # ë‹¤ìŒ ì²­í¬ì˜ ì‹œì‘ ë¶€ë¶„ ì¶”ì¶œ
            next_words = next_chunk.content.split()
            overlap_start = next_words[:self.chunk_overlap] if len(next_words) > self.chunk_overlap else next_words
            
            # ì˜¤ë²„ë© ì •ë³´ ì¶”ê°€
            current_chunk.overlap_with_next = True
            next_chunk.overlap_with_previous = True
            
            # ë©”íƒ€ë°ì´í„°ì— ì˜¤ë²„ë© ì •ë³´ ì¶”ê°€
            current_chunk.metadata['overlap_next'] = ' '.join(overlap_start)
            next_chunk.metadata['overlap_prev'] = ' '.join(overlap_words)
        
        return chunks
    
    def _create_chunk(
        self, 
        content: str, 
        doc_id: str, 
        chunk_index: int, 
        chunk_type: str,
        metadata: Dict = None
    ) -> Chunk:
        """ì²­í¬ ê°ì²´ ìƒì„±"""
        return Chunk(
            content=content.strip(),
            chunk_id=f"{doc_id}_chunk_{chunk_index}",
            doc_id=doc_id,
            chunk_index=chunk_index,
            chunk_type=chunk_type,
            metadata=metadata or {},
            token_count=self._count_tokens(content),
            char_count=len(content)
        )
    
    def _count_tokens(self, text: str) -> int:
        """í† í° ìˆ˜ ê³„ì‚°"""
        return len(self.tokenizer.encode(text))
    
    def _extract_all_text(self, processed_data: Dict) -> str:
        """ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ"""
        all_text = []
        
        # í‘œ í…ìŠ¤íŠ¸
        for table in processed_data.get('tables', []):
            all_text.append(table.get('formatted_text', ''))
        
        # ë‹¨ë½ í…ìŠ¤íŠ¸
        for para in processed_data.get('paragraphs', []):
            all_text.append(para.get('cleaned', ''))
        
        return '\n\n'.join(all_text)
    
    def print_chunking_summary(self, chunks: List[Chunk]):
        """ì²­í‚¹ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“¦ ì²­í‚¹ ê²°ê³¼")
        print("="*50)
        print(f"ì´ ì²­í¬ ìˆ˜: {len(chunks)}")
        
        # ì²­í¬ íƒ€ì…ë³„ í†µê³„
        chunk_types = {}
        for chunk in chunks:
            chunk_types[chunk.chunk_type] = chunk_types.get(chunk.chunk_type, 0) + 1
        
        print("\nì²­í¬ íƒ€ì…ë³„ ë¶„í¬:")
        for chunk_type, count in chunk_types.items():
            print(f"  - {chunk_type}: {count}ê°œ")
        
        # í† í° í†µê³„
        token_counts = [c.token_count for c in chunks]
        if token_counts:
            print(f"\ní† í° í†µê³„:")
            print(f"  - í‰ê· : {sum(token_counts) / len(token_counts):.1f}")
            print(f"  - ìµœì†Œ: {min(token_counts)}")
            print(f"  - ìµœëŒ€: {max(token_counts)}")
        
        # ìƒ˜í”Œ ì²­í¬
        if chunks:
            print(f"\nğŸ“„ ì²« ë²ˆì§¸ ì²­í¬ ìƒ˜í”Œ:")
            print(f"  ID: {chunks[0].chunk_id}")
            print(f"  íƒ€ì…: {chunks[0].chunk_type}")
            print(f"  í† í°: {chunks[0].token_count}")
            print(f"  ë‚´ìš©: {chunks[0].content[:100]}...")
        
        print("="*50)
    
    def save_chunks(self, chunks: List[Chunk], output_path: str):
        """ì²­í¬ ì €ì¥"""
        import json
        from pathlib import Path
        
        output_path = Path(output_path)
        
        # JSONìœ¼ë¡œ ì €ì¥
        chunks_data = []
        for chunk in chunks:
            chunks_data.append({
                'chunk_id': chunk.chunk_id,
                'doc_id': chunk.doc_id,
                'chunk_index': chunk.chunk_index,
                'content': chunk.content,
                'chunk_type': chunk.chunk_type,
                'metadata': chunk.metadata,
                'token_count': chunk.token_count,
                'char_count': chunk.char_count
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Chunks saved to {output_path}")
