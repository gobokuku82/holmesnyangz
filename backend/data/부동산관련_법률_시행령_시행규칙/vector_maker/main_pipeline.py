"""
ë¶€ë™ì‚° ë²•ë¥  ë¬¸ì„œ ë²¡í„°DB êµ¬ì¶• ë©”ì¸ íŒŒì´í”„ë¼ì¸
"""
import os
import sys
from pathlib import Path
import logging
from typing import List, Dict, Any
import json
from datetime import datetime

# ëª¨ë“ˆ ì„í¬íŠ¸
from config import *
from document_analyzer import DocumentAnalyzer
from document_preprocessor import DocumentPreprocessor
from document_chunker import DocumentChunker, Chunk
from vectordb_builder import VectorDBBuilder, EmbeddingValidator

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class DocumentPipeline:
    """ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self):
        self.analyzer = DocumentAnalyzer()
        self.preprocessor = DocumentPreprocessor()
        self.chunker = DocumentChunker(
            chunk_size=CHUNK_SIZE,
            chunk_overlap=CHUNK_OVERLAP,
            min_chunk_size=MIN_CHUNK_SIZE
        )
        self.vectordb = VectorDBBuilder(
            embedding_model=EMBEDDING_MODEL,
            collection_name=COLLECTION_NAME,
            persist_directory=CHROMA_PERSIST_DIR
        )
        
        self.processing_stats = []
    
    def process_single_document(
        self, 
        file_path: str,
        doc_id: str = None,
        test_mode: bool = False
    ) -> Dict[str, Any]:
        """ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬"""
        
        file_path = Path(file_path)
        if not doc_id:
            doc_id = file_path.stem
        
        logger.info(f"Processing document: {file_path.name}")
        
        stats = {
            'doc_id': doc_id,
            'file_name': file_path.name,
            'start_time': datetime.now().isoformat(),
            'stages': {}
        }
        
        try:
            # 1. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
            print(f"\nğŸ“‹ Processing: {file_path.name}")
            print("-" * 40)
            
            structure = self.analyzer.analyze(file_path)
            self.analyzer.print_structure_summary(structure)
            stats['stages']['analysis'] = {
                'doc_type': structure.doc_type,
                'table_count': structure.table_count,
                'status': 'success'
            }
            
            # 2. ì „ì²˜ë¦¬
            processed_data = self.preprocessor.preprocess(
                file_path,
                structure_info=structure.__dict__
            )
            self.preprocessor.print_preprocessing_summary(processed_data)
            
            # ì „ì²˜ë¦¬ ê²°ê³¼ ì €ì¥
            processed_path = PROCESSED_DIR / f"{doc_id}_processed"
            self.preprocessor.save_processed_data(processed_data, str(processed_path))
            stats['stages']['preprocessing'] = {
                'table_count': len(processed_data['tables']),
                'paragraph_count': len(processed_data['paragraphs']),
                'status': 'success'
            }
            
            # 3. ì²­í‚¹
            chunks = self.chunker.chunk_document(processed_data, doc_id)
            self.chunker.print_chunking_summary(chunks)
            
            # ì²­í¬ ì €ì¥
            chunks_path = CHUNKS_DIR / f"{doc_id}_chunks.json"
            self.chunker.save_chunks(chunks, str(chunks_path))
            stats['stages']['chunking'] = {
                'chunk_count': len(chunks),
                'avg_tokens': sum(c.token_count for c in chunks) / len(chunks) if chunks else 0,
                'status': 'success'
            }
            
            # í…ŒìŠ¤íŠ¸ ëª¨ë“œë©´ ì—¬ê¸°ì„œ ì¤‘ë‹¨
            if test_mode:
                print("\nâš ï¸ Test mode: Stopping before embedding")
                self._run_chunking_test(chunks)
                stats['test_mode'] = True
                return stats
            
            # 4. ì„ë² ë”© ìƒì„±
            print("\nğŸ”„ Creating embeddings...")
            embeddings = self.vectordb.embed_chunks(chunks)
            stats['stages']['embedding'] = {
                'embedding_count': len(embeddings),
                'embedding_dim': embeddings.shape[1],
                'status': 'success'
            }
            
            # 5. ì„ë² ë”© ê²€ì¦
            validator = EmbeddingValidator(embeddings, chunks)
            validator.print_validation_report()
            
            # 6. ë²¡í„°DB ì €ì¥
            doc_metadata = {
                'file_name': file_path.name,
                'doc_type': structure.doc_type,
                'processed_date': datetime.now().isoformat()
            }
            self.vectordb.add_to_vectordb(chunks, embeddings, doc_metadata)
            self.vectordb.print_db_summary()
            stats['stages']['vectordb'] = {
                'documents_added': len(chunks),
                'status': 'success'
            }
            
            stats['end_time'] = datetime.now().isoformat()
            stats['status'] = 'success'
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to process {file_path.name}: {e}")
            stats['error'] = str(e)
            stats['status'] = 'failed'
            return stats
    
    def process_batch(
        self, 
        file_paths: List[str],
        test_mode: bool = False
    ) -> List[Dict]:
        """ë°°ì¹˜ ë¬¸ì„œ ì²˜ë¦¬"""
        
        results = []
        
        for i, file_path in enumerate(file_paths):
            print(f"\n{'='*50}")
            print(f"Processing {i+1}/{len(file_paths)}")
            print('='*50)
            
            result = self.process_single_document(
                file_path,
                test_mode=test_mode
            )
            results.append(result)
            self.processing_stats.append(result)
        
        self._print_batch_summary(results)
        return results
    
    def _run_chunking_test(self, chunks: List[Chunk]):
        """ì²­í‚¹ ê²°ê³¼ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*50)
        print("ğŸ§ª ì²­í‚¹ í…ŒìŠ¤íŠ¸")
        print("="*50)
        
        # ìƒ˜í”Œ ì²­í¬ ìƒì„¸ ì¶œë ¥
        sample_count = min(3, len(chunks))
        print(f"\nìƒ˜í”Œ ì²­í¬ ({sample_count}ê°œ):")
        
        for i in range(sample_count):
            chunk = chunks[i]
            print(f"\n--- ì²­í¬ {i+1} ---")
            print(f"ID: {chunk.chunk_id}")
            print(f"íƒ€ì…: {chunk.chunk_type}")
            print(f"í† í° ìˆ˜: {chunk.token_count}")
            print(f"ë¬¸ì ìˆ˜: {chunk.char_count}")
            print(f"ì˜¤ë²„ë©: ì´ì „={chunk.overlap_with_previous}, ë‹¤ìŒ={chunk.overlap_with_next}")
            print(f"ë‚´ìš©:\n{chunk.content[:300]}...")
            
            if chunk.metadata:
                print(f"ë©”íƒ€ë°ì´í„°: {list(chunk.metadata.keys())}")
    
    def run_retrieval_test(self, test_queries: List[str] = None):
        """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        
        if test_queries is None:
            # ê¸°ë³¸ í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬
            test_queries = [
                "ê³µì¸ì¤‘ê°œì‚¬ë²•",
                "ë¶€ë™ì‚° ê±°ë˜ ì‹ ê³ ",
                "ì„ëŒ€ì°¨ ê³„ì•½",
                "ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²•",
                "ë§¤ë§¤ ê³„ì•½ ê´€ë ¨ ë²•ë¥ ",
                "ë¶€ë™ì‚°ë“±ê¸°ë²•",
                "ê±´ì¶•ë¬¼ ê´€ë ¨ ê·œì •"
            ]
        
        self.vectordb.test_retrieval(test_queries)
    
    def _print_batch_summary(self, results: List[Dict]):
        """ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½"""
        print("\n" + "="*50)
        print("ğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½")
        print("="*50)
        
        success_count = sum(1 for r in results if r.get('status') == 'success')
        failed_count = len(results) - success_count
        
        print(f"ì´ ì²˜ë¦¬: {len(results)}ê°œ")
        print(f"ì„±ê³µ: {success_count}ê°œ")
        print(f"ì‹¤íŒ¨: {failed_count}ê°œ")
        
        if failed_count > 0:
            print("\nì‹¤íŒ¨í•œ ë¬¸ì„œ:")
            for r in results:
                if r.get('status') == 'failed':
                    print(f"  - {r['file_name']}: {r.get('error', 'Unknown error')}")
        
        # í†µê³„
        total_chunks = sum(
            r['stages'].get('chunking', {}).get('chunk_count', 0)
            for r in results
            if 'stages' in r
        )
        
        print(f"\nì´ ìƒì„±ëœ ì²­í¬: {total_chunks}ê°œ")
    
    def save_processing_stats(self, output_path: str = None):
        """ì²˜ë¦¬ í†µê³„ ì €ì¥"""
        if not output_path:
            output_path = BASE_DIR / f"processing_stats_{datetime.now():%Y%m%d_%H%M%S}.json"
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(self.processing_stats, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Processing stats saved to {output_path}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    
    # íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
    pipeline = DocumentPipeline()
    
    # ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ ì˜ˆì œ
    # file_path = RAW_DIR / "example_document.docx"
    # stats = pipeline.process_single_document(
    #     file_path,
    #     test_mode=True  # í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰
    # )
    
    # ë°°ì¹˜ ì²˜ë¦¬ ì˜ˆì œ
    # doc_files = list(RAW_DIR.glob("*.docx"))
    # results = pipeline.process_batch(doc_files[:5], test_mode=False)
    
    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    # pipeline.run_retrieval_test()
    
    # í†µê³„ ì €ì¥
    # pipeline.save_processing_stats()
    
    print("\nâœ… Pipeline ready! Use the following methods:")
    print("  - pipeline.process_single_document(file_path, test_mode=True)")
    print("  - pipeline.process_batch(file_paths)")
    print("  - pipeline.run_retrieval_test()")
    
    return pipeline


if __name__ == "__main__":
    pipeline = main()
