"""
ì„ë² ë”© ë° ChromaDB ë²¡í„° DB êµ¬ì¶• ëª¨ë“ˆ
- Kure-v1 (BGE-M3 ê¸°ë°˜) ì„ë² ë”©
- ChromaDB ì €ì¥ ë° ê´€ë¦¬
"""
import os
from typing import List, Dict, Any, Optional
from pathlib import Path
import chromadb
from chromadb.config import Settings
from sentence_transformers import SentenceTransformer
import numpy as np
import logging
from tqdm import tqdm
import json

logger = logging.getLogger(__name__)

class VectorDBBuilder:
    """ë²¡í„° DB êµ¬ì¶• í´ë˜ìŠ¤"""
    
    def __init__(
        self,
        embedding_model: str = "BAAI/bge-m3",
        collection_name: str = "real_estate_law",
        persist_directory: str = "./vectordb/chroma",
        distance_metric: str = "cosine"
    ):
        self.embedding_model_name = embedding_model
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.distance_metric = distance_metric
        
        # ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
        logger.info(f"Loading embedding model: {embedding_model}")
        self.embedding_model = SentenceTransformer(embedding_model)
        
        # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
        self.client = chromadb.PersistentClient(
            path=persist_directory,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ë¡œë“œ
        self._init_collection()
    
    def _init_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            # ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì˜µì…˜ (í•„ìš”ì‹œ)
            # self.client.delete_collection(self.collection_name)
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
            self.collection = self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": self.distance_metric}
            )
            
            logger.info(f"Collection '{self.collection_name}' initialized")
            
        except Exception as e:
            logger.error(f"Failed to initialize collection: {e}")
            raise
    
    def embed_chunks(self, chunks: List[Dict]) -> np.ndarray:
        """ì²­í¬ ì„ë² ë”© ìƒì„±"""
        logger.info(f"Embedding {len(chunks)} chunks...")
        
        # í…ìŠ¤íŠ¸ ì¶”ì¶œ
        texts = []
        for chunk in chunks:
            if isinstance(chunk, dict):
                text = chunk.get('content', '')
            else:
                text = chunk.content
            texts.append(text)
        
        # ë°°ì¹˜ ì„ë² ë”©
        embeddings = []
        batch_size = 32
        
        for i in tqdm(range(0, len(texts), batch_size), desc="Embedding chunks"):
            batch_texts = texts[i:i+batch_size]
            batch_embeddings = self.embedding_model.encode(
                batch_texts,
                normalize_embeddings=True,  # BGE-M3ëŠ” ì •ê·œí™” ê¶Œì¥
                show_progress_bar=False
            )
            embeddings.extend(batch_embeddings)
        
        return np.array(embeddings)
    
    def add_to_vectordb(
        self, 
        chunks: List[Dict], 
        embeddings: Optional[np.ndarray] = None,
        doc_metadata: Dict = None
    ):
        """ë²¡í„° DBì— ì²­í¬ ì¶”ê°€"""
        
        # ì„ë² ë”©ì´ ì œê³µë˜ì§€ ì•Šìœ¼ë©´ ìƒì„±
        if embeddings is None:
            embeddings = self.embed_chunks(chunks)
        
        # ChromaDBì— ì¶”ê°€í•  ë°ì´í„° ì¤€ë¹„
        ids = []
        documents = []
        metadatas = []
        embeddings_list = []
        
        for i, chunk in enumerate(chunks):
            # ì²­í¬ ë°ì´í„° ì¶”ì¶œ
            if isinstance(chunk, dict):
                chunk_id = chunk.get('chunk_id', f"chunk_{i}")
                content = chunk.get('content', '')
                metadata = chunk.get('metadata', {})
                chunk_type = chunk.get('chunk_type', 'unknown')
                doc_id = chunk.get('doc_id', 'unknown')
                chunk_index = chunk.get('chunk_index', i)
            else:
                chunk_id = chunk.chunk_id
                content = chunk.content
                metadata = chunk.metadata
                chunk_type = chunk.chunk_type
                doc_id = chunk.doc_id
                chunk_index = chunk.chunk_index
            
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            chunk_metadata = {
                'doc_id': doc_id,
                'chunk_index': chunk_index,
                'chunk_type': chunk_type,
                'char_count': len(content),
                **metadata  # ì¶”ê°€ ë©”íƒ€ë°ì´í„°
            }
            
            # ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶”ê°€
            if doc_metadata:
                chunk_metadata.update({
                    f'doc_{k}': v for k, v in doc_metadata.items()
                })
            
            ids.append(chunk_id)
            documents.append(content)
            metadatas.append(chunk_metadata)
            embeddings_list.append(embeddings[i].tolist())
        
        # ChromaDBì— ì¶”ê°€
        try:
            self.collection.add(
                ids=ids,
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings_list
            )
            
            logger.info(f"Added {len(chunks)} chunks to vector database")
            
        except Exception as e:
            logger.error(f"Failed to add chunks to database: {e}")
            raise
    
    def search(
        self, 
        query: str, 
        n_results: int = 5,
        filter_dict: Dict = None
    ) -> Dict[str, Any]:
        """ë²¡í„° ê²€ìƒ‰"""
        
        # ì¿¼ë¦¬ ì„ë² ë”©
        query_embedding = self.embedding_model.encode(
            [query],
            normalize_embeddings=True
        )[0]
        
        # ChromaDB ê²€ìƒ‰
        results = self.collection.query(
            query_embeddings=[query_embedding.tolist()],
            n_results=n_results,
            where=filter_dict if filter_dict else None,
            include=['documents', 'metadatas', 'distances']
        )
        
        # ê²°ê³¼ í¬ë§·íŒ…
        formatted_results = {
            'query': query,
            'results': []
        }
        
        for i in range(len(results['ids'][0])):
            formatted_results['results'].append({
                'id': results['ids'][0][i],
                'document': results['documents'][0][i],
                'metadata': results['metadatas'][0][i],
                'distance': results['distances'][0][i],
                'similarity': 1 - results['distances'][0][i]  # ì½”ì‚¬ì¸ ê±°ë¦¬ -> ìœ ì‚¬ë„
            })
        
        return formatted_results
    
    def test_retrieval(self, test_queries: List[str], n_results: int = 3):
        """ê²€ìƒ‰ í…ŒìŠ¤íŠ¸"""
        print("\n" + "="*50)
        print("ğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸")
        print("="*50)
        
        for query in test_queries:
            print(f"\nğŸ“Œ Query: {query}")
            print("-"*40)
            
            results = self.search(query, n_results=n_results)
            
            for i, result in enumerate(results['results'], 1):
                print(f"\nê²°ê³¼ {i}:")
                print(f"  ìœ ì‚¬ë„: {result['similarity']:.3f}")
                print(f"  ì²­í¬ íƒ€ì…: {result['metadata'].get('chunk_type', 'N/A')}")
                print(f"  ë¬¸ì„œ ID: {result['metadata'].get('doc_id', 'N/A')}")
                print(f"  ë‚´ìš©: {result['document'][:150]}...")
        
        print("="*50)
    
    def get_collection_stats(self) -> Dict[str, Any]:
        """ì»¬ë ‰ì…˜ í†µê³„"""
        count = self.collection.count()
        
        # ë©”íƒ€ë°ì´í„° ìƒ˜í”Œë§
        sample = self.collection.peek(limit=10)
        
        stats = {
            'total_documents': count,
            'collection_name': self.collection_name,
            'embedding_model': self.embedding_model_name,
            'distance_metric': self.distance_metric,
            'sample_metadata': sample['metadatas'] if sample else []
        }
        
        return stats
    
    def print_db_summary(self):
        """DB ìš”ì•½ ì¶œë ¥"""
        stats = self.get_collection_stats()
        
        print("\n" + "="*50)
        print("ğŸ—„ï¸ ë²¡í„° DB ìš”ì•½")
        print("="*50)
        print(f"ì»¬ë ‰ì…˜ ì´ë¦„: {stats['collection_name']}")
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {stats['total_documents']}")
        print(f"ì„ë² ë”© ëª¨ë¸: {stats['embedding_model']}")
        print(f"ê±°ë¦¬ ë©”íŠ¸ë¦­: {stats['distance_metric']}")
        
        # ì²­í¬ íƒ€ì… ë¶„í¬
        if stats['sample_metadata']:
            chunk_types = {}
            for metadata in stats['sample_metadata']:
                chunk_type = metadata.get('chunk_type', 'unknown')
                chunk_types[chunk_type] = chunk_types.get(chunk_type, 0) + 1
            
            print("\nìƒ˜í”Œ ì²­í¬ íƒ€ì… ë¶„í¬:")
            for chunk_type, count in chunk_types.items():
                print(f"  - {chunk_type}: {count}ê°œ")
        
        print("="*50)
    
    def export_collection(self, output_path: str):
        """ì»¬ë ‰ì…˜ ë‚´ë³´ë‚´ê¸°"""
        output_path = Path(output_path)
        
        # ì „ì²´ ë°ì´í„° ê°€ì ¸ì˜¤ê¸°
        all_data = self.collection.get()
        
        # JSONìœ¼ë¡œ ì €ì¥
        export_data = {
            'collection_name': self.collection_name,
            'embedding_model': self.embedding_model_name,
            'total_documents': len(all_data['ids']),
            'documents': []
        }
        
        for i in range(len(all_data['ids'])):
            export_data['documents'].append({
                'id': all_data['ids'][i],
                'document': all_data['documents'][i],
                'metadata': all_data['metadatas'][i]
            })
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(export_data, f, ensure_ascii=False, indent=2)
        
        logger.info(f"Collection exported to {output_path}")
    
    def reset_collection(self):
        """ì»¬ë ‰ì…˜ ì´ˆê¸°í™”"""
        try:
            self.client.delete_collection(self.collection_name)
            self._init_collection()
            logger.info(f"Collection '{self.collection_name}' reset")
        except Exception as e:
            logger.error(f"Failed to reset collection: {e}")
            raise


class EmbeddingValidator:
    """ì„ë² ë”© í’ˆì§ˆ ê²€ì¦"""
    
    def __init__(self, embeddings: np.ndarray, chunks: List[Dict]):
        self.embeddings = embeddings
        self.chunks = chunks
    
    def validate_dimensions(self) -> bool:
        """ì°¨ì› ê²€ì¦"""
        expected_dim = self.embeddings.shape[1]
        for i, emb in enumerate(self.embeddings):
            if len(emb) != expected_dim:
                logger.error(f"Dimension mismatch at chunk {i}")
                return False
        return True
    
    def check_similarity_distribution(self) -> Dict[str, float]:
        """ìœ ì‚¬ë„ ë¶„í¬ í™•ì¸"""
        # ëœë¤ ìƒ˜í”Œë§
        sample_size = min(100, len(self.embeddings))
        sample_indices = np.random.choice(
            len(self.embeddings), 
            sample_size, 
            replace=False
        )
        
        # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
        similarities = []
        for i in range(sample_size):
            for j in range(i+1, sample_size):
                sim = np.dot(
                    self.embeddings[sample_indices[i]], 
                    self.embeddings[sample_indices[j]]
                )
                similarities.append(sim)
        
        return {
            'mean': np.mean(similarities),
            'std': np.std(similarities),
            'min': np.min(similarities),
            'max': np.max(similarities)
        }
    
    def find_duplicate_embeddings(self, threshold: float = 0.99) -> List[Tuple[int, int]]:
        """ì¤‘ë³µ ì„ë² ë”© ì°¾ê¸°"""
        duplicates = []
        
        for i in range(len(self.embeddings)):
            for j in range(i+1, len(self.embeddings)):
                sim = np.dot(self.embeddings[i], self.embeddings[j])
                if sim > threshold:
                    duplicates.append((i, j))
        
        return duplicates
    
    def print_validation_report(self):
        """ê²€ì¦ ë³´ê³ ì„œ ì¶œë ¥"""
        print("\n" + "="*50)
        print("âœ… ì„ë² ë”© ê²€ì¦ ë³´ê³ ì„œ")
        print("="*50)
        
        # ì°¨ì› ê²€ì¦
        dim_valid = self.validate_dimensions()
        print(f"ì°¨ì› ì¼ê´€ì„±: {'âœ“' if dim_valid else 'âœ—'}")
        print(f"ì„ë² ë”© ì°¨ì›: {self.embeddings.shape[1]}")
        
        # ìœ ì‚¬ë„ ë¶„í¬
        sim_dist = self.check_similarity_distribution()
        print(f"\nìœ ì‚¬ë„ ë¶„í¬:")
        print(f"  - í‰ê· : {sim_dist['mean']:.3f}")
        print(f"  - í‘œì¤€í¸ì°¨: {sim_dist['std']:.3f}")
        print(f"  - ìµœì†Œ: {sim_dist['min']:.3f}")
        print(f"  - ìµœëŒ€: {sim_dist['max']:.3f}")
        
        # ì¤‘ë³µ ê²€ì‚¬
        duplicates = self.find_duplicate_embeddings()
        print(f"\nì¤‘ë³µ ì„ë² ë”©: {len(duplicates)}ê°œ")
        
        if duplicates[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
            print("  ì¤‘ë³µ ì˜ˆì‹œ:")
            for i, j in duplicates[:3]:
                print(f"    - ì²­í¬ {i} <-> ì²­í¬ {j}")
        
        print("="*50)
