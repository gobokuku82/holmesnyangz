# ë¶€ë™ì‚° ë²•ë¥  ë¬¸ì„œ ë²¡í„°DB êµ¬ì¶• ì‹œìŠ¤í…œ

## ğŸ“ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
.
â”œâ”€â”€ config.py                 # ì „ì²´ ì„¤ì • íŒŒì¼
â”œâ”€â”€ document_analyzer.py      # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ëª¨ë“ˆ
â”œâ”€â”€ document_preprocessor.py  # ì „ì²˜ë¦¬ ëª¨ë“ˆ
â”œâ”€â”€ document_chunker.py       # ì²­í‚¹ ëª¨ë“ˆ
â”œâ”€â”€ vectordb_builder.py       # ì„ë² ë”© ë° ë²¡í„°DB ëª¨ë“ˆ
â”œâ”€â”€ main_pipeline.py          # ë©”ì¸ íŒŒì´í”„ë¼ì¸
â””â”€â”€ data/
    â”œâ”€â”€ raw/                  # ì›ë³¸ ë¬¸ì„œ
    â”œâ”€â”€ processed/            # ì „ì²˜ë¦¬ëœ ë¬¸ì„œ
    â”œâ”€â”€ chunks/               # ì²­í‚¹ ê²°ê³¼
    â””â”€â”€ vectordb/
        â””â”€â”€ chroma/           # ChromaDB ì €ì¥ì†Œ
```

## ğŸš€ ë¹ ë¥¸ ì‹œì‘

### 1. í•„ìš” íŒ¨í‚¤ì§€ ì„¤ì¹˜

```bash
pip install python-docx pandas numpy
pip install sentence-transformers chromadb
pip install tiktoken tqdm
```

### 2. ê¸°ë³¸ ì‚¬ìš©ë²•

```python
from main_pipeline import DocumentPipeline

# íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
pipeline = DocumentPipeline()

# ë‹¨ì¼ ë¬¸ì„œ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
stats = pipeline.process_single_document(
    "path/to/document.docx",
    test_mode=True  # ì„ë² ë”© ì „ì— ì¤‘ë‹¨í•˜ê³  ì²­í‚¹ ê²°ê³¼ í™•ì¸
)

# ë‹¨ì¼ ë¬¸ì„œ ì „ì²´ ì²˜ë¦¬
stats = pipeline.process_single_document(
    "path/to/document.docx",
    test_mode=False
)

# ë°°ì¹˜ ì²˜ë¦¬
doc_files = ["doc1.docx", "doc2.docx", "doc3.docx"]
results = pipeline.process_batch(doc_files)

# ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
test_queries = [
    "ê³µì¸ì¤‘ê°œì‚¬ë²•",
    "ë¶€ë™ì‚° ê±°ë˜ ì‹ ê³ ",
    "ì„ëŒ€ì°¨ ê³„ì•½"
]
pipeline.run_retrieval_test(test_queries)
```

## ğŸ“‹ ë‹¨ê³„ë³„ ì²˜ë¦¬

### 1. ë¬¸ì„œ êµ¬ì¡° ë¶„ì„

```python
from document_analyzer import DocumentAnalyzer

analyzer = DocumentAnalyzer()
structure = analyzer.analyze("document.docx")
analyzer.print_structure_summary(structure)
```

**ì¶œë ¥ ì •ë³´:**
- ë¬¸ì„œ íƒ€ì… (table, text, mixed)
- í‘œ ê°œìˆ˜ ë° êµ¬ì¡°
- ë©”íƒ€ë°ì´í„° (ë²•ë ¹ ë²ˆí˜¸, ë‚ ì§œ ë“±)

### 2. ì „ì²˜ë¦¬

```python
from document_preprocessor import DocumentPreprocessor

preprocessor = DocumentPreprocessor()
processed_data = preprocessor.preprocess("document.docx")
preprocessor.save_processed_data(processed_data, "output_path")
```

**ì „ì²˜ë¦¬ ê¸°ëŠ¥:**
- í‘œ ë°ì´í„° ì •ê·œí™”
- í…ìŠ¤íŠ¸ í´ë¦¬ë‹
- ë²•ë¥  ì°¸ì¡° ì¶”ì¶œ
- êµ¬ì¡° ì •ë³´ ë³´ì¡´

### 3. ì²­í‚¹

```python
from document_chunker import DocumentChunker

chunker = DocumentChunker(
    chunk_size=500,      # í† í° ê¸°ì¤€
    chunk_overlap=50,    # ì˜¤ë²„ë© í† í°
    min_chunk_size=100   # ìµœì†Œ ì²­í¬ í¬ê¸°
)

chunks = chunker.chunk_document(processed_data, doc_id="doc001")
chunker.print_chunking_summary(chunks)
```

**ì²­í‚¹ íŠ¹ì§•:**
- ì˜ë¯¸ ë‹¨ìœ„ ë³´ì¡´
- í‘œ êµ¬ì¡° ë³´ì¡´
- ì˜¤ë²„ë© ì²˜ë¦¬
- ë©”íƒ€ë°ì´í„° ìœ ì§€

### 4. ì„ë² ë”© ë° ë²¡í„°DB

```python
from vectordb_builder import VectorDBBuilder

vectordb = VectorDBBuilder(
    embedding_model="BAAI/bge-m3",
    collection_name="real_estate_law"
)

# ì„ë² ë”© ìƒì„±
embeddings = vectordb.embed_chunks(chunks)

# ë²¡í„°DB ì €ì¥
vectordb.add_to_vectordb(chunks, embeddings)

# ê²€ìƒ‰
results = vectordb.search("ë¶€ë™ì‚° ê±°ë˜ ì‹ ê³ ", n_results=5)
```

## ğŸ§ª í…ŒìŠ¤íŠ¸ ë° ê²€ì¦

### ì²­í‚¹ í…ŒìŠ¤íŠ¸

```python
# test_mode=Trueë¡œ ì‹¤í–‰í•˜ë©´ ì²­í‚¹ ê²°ê³¼ë¥¼ ìƒì„¸íˆ í™•ì¸
pipeline.process_single_document("document.docx", test_mode=True)
```

### ì„ë² ë”© ê²€ì¦

```python
from vectordb_builder import EmbeddingValidator

validator = EmbeddingValidator(embeddings, chunks)
validator.print_validation_report()
```

ê²€ì¦ í•­ëª©:
- ì°¨ì› ì¼ê´€ì„±
- ìœ ì‚¬ë„ ë¶„í¬
- ì¤‘ë³µ ì„ë² ë”©

### ê²€ìƒ‰ í’ˆì§ˆ í…ŒìŠ¤íŠ¸

```python
# ë‹¤ì–‘í•œ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
test_queries = [
    "íŠ¹ì • ë²•ë¥  ì¡°í•­",
    "ë³µì¡í•œ ì§ˆì˜",
    "í‚¤ì›Œë“œ ì¡°í•©"
]
pipeline.run_retrieval_test(test_queries)
```

## âš™ï¸ ì„¤ì • ì»¤ìŠ¤í„°ë§ˆì´ì§•

`config.py` íŒŒì¼ì—ì„œ ë‹¤ìŒ ì„¤ì •ì„ ì¡°ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:

```python
# ì²­í‚¹ ì„¤ì •
CHUNK_SIZE = 500        # ì²­í¬ í¬ê¸° (í† í°)
CHUNK_OVERLAP = 50      # ì˜¤ë²„ë© í¬ê¸°
MIN_CHUNK_SIZE = 100    # ìµœì†Œ ì²­í¬ í¬ê¸°

# ì„ë² ë”© ì„¤ì •
EMBEDDING_MODEL = "BAAI/bge-m3"  # ì„ë² ë”© ëª¨ë¸
EMBEDDING_DIMENSION = 1024       # ì„ë² ë”© ì°¨ì›

# ChromaDB ì„¤ì •
COLLECTION_NAME = "real_estate_law"
DISTANCE_METRIC = "cosine"
```

## ğŸ“Š ë¬¸ì„œë³„ íŠ¹ë³„ ì²˜ë¦¬

### í‘œê°€ ë§ì€ ë¬¸ì„œ

```python
# í‘œ ì¤‘ì‹¬ ì²˜ë¦¬ ì„¤ì •
chunker = DocumentChunker(
    chunk_size=800,  # í‘œë¥¼ ìœ„í•´ í¬ê¸° ì¦ê°€
    chunk_overlap=0   # í‘œëŠ” ì˜¤ë²„ë© ë¶ˆí•„ìš”
)
```

### ê¸´ í…ìŠ¤íŠ¸ ë¬¸ì„œ

```python
# í…ìŠ¤íŠ¸ ì¤‘ì‹¬ ì²˜ë¦¬ ì„¤ì •
chunker = DocumentChunker(
    chunk_size=400,   # ì‘ì€ ì²­í¬
    chunk_overlap=100  # ë” ë§ì€ ì˜¤ë²„ë©
)
```

## ğŸ” ê³ ê¸‰ ê²€ìƒ‰ ê¸°ëŠ¥

### í•„í„°ë§ ê²€ìƒ‰

```python
# íŠ¹ì • ë¬¸ì„œ íƒ€ì…ë§Œ ê²€ìƒ‰
results = vectordb.search(
    query="ë¶€ë™ì‚° ê±°ë˜",
    n_results=5,
    filter_dict={"chunk_type": "table"}
)
```

### ìœ ì‚¬ë„ ì„ê³„ê°’ ì„¤ì •

```python
results = vectordb.search(query, n_results=10)
# ìœ ì‚¬ë„ 0.7 ì´ìƒë§Œ í•„í„°ë§
filtered = [r for r in results['results'] if r['similarity'] > 0.7]
```

## ğŸ“ˆ ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§

```python
# ì²˜ë¦¬ í†µê³„ ì €ì¥
pipeline.save_processing_stats()

# ë²¡í„°DB í†µê³„
stats = vectordb.get_collection_stats()
print(f"Total documents: {stats['total_documents']}")
```

## ğŸ› ï¸ ë¬¸ì œ í•´ê²°

### ë©”ëª¨ë¦¬ ë¶€ì¡±
- `chunk_size` ê°ì†Œ
- ë°°ì¹˜ í¬ê¸° ì¡°ì •
- ë¬¸ì„œë¥¼ ë” ì‘ì€ ë‹¨ìœ„ë¡œ ë¶„í• 

### ê²€ìƒ‰ í’ˆì§ˆ ì €í•˜
- `chunk_overlap` ì¦ê°€
- ì²­í‚¹ ì „ëµ ì¡°ì •
- ì „ì²˜ë¦¬ ê°•í™”

### ì²˜ë¦¬ ì†ë„
- ë°°ì¹˜ ì²˜ë¦¬ í™œìš©
- ì„ë² ë”© ìºì‹±
- ë³‘ë ¬ ì²˜ë¦¬ ì ìš©

## ğŸ“ ì˜ˆì œ: ì „ì²´ ì›Œí¬í”Œë¡œìš°

```python
# 1. ì¤€ë¹„
from pathlib import Path
from main_pipeline import DocumentPipeline

pipeline = DocumentPipeline()

# 2. ë¬¸ì„œ ì¤€ë¹„
doc_dir = Path("./documents")
doc_files = list(doc_dir.glob("*.docx"))

# 3. ë°°ì¹˜ ì²˜ë¦¬ (í…ŒìŠ¤íŠ¸ ëª¨ë“œ)
print("Testing with first document...")
pipeline.process_single_document(
    doc_files[0],
    test_mode=True
)

# 4. í™•ì¸ í›„ ì „ì²´ ì²˜ë¦¬
input("Press Enter to continue with full processing...")
results = pipeline.process_batch(doc_files)

# 5. ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
queries = [
    "ê³µì¸ì¤‘ê°œì‚¬ ìê²©",
    "ë¶€ë™ì‚° ê±°ë˜ì‹ ê³  ì˜ë¬´",
    "ì„ëŒ€ì°¨ ë³´í˜¸ë²•"
]
pipeline.run_retrieval_test(queries)

# 6. ê²°ê³¼ ì €ì¥
pipeline.save_processing_stats("./results/stats.json")
vectordb.export_collection("./results/collection.json")
```

## ğŸ“Œ ì£¼ì˜ì‚¬í•­

1. **ë¬¸ì„œ í˜•ì‹**: í˜„ì¬ `.docx` í˜•ì‹ë§Œ ì§€ì›
2. **í‘œ ì²˜ë¦¬**: ë³µì¡í•œ ë³‘í•© ì…€ì€ ë‹¨ìˆœí™”ë¨
3. **ë©”ëª¨ë¦¬**: ëŒ€ìš©ëŸ‰ ë¬¸ì„œëŠ” ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì²˜ë¦¬
4. **ì„ë² ë”© ëª¨ë¸**: BGE-M3ëŠ” í•œêµ­ì–´ ì§€ì›ì´ ìš°ìˆ˜í•˜ì§€ë§Œ ë‹¤ë¥¸ ëª¨ë¸ë¡œ ë³€ê²½ ê°€ëŠ¥

## ğŸš¦ ë‹¤ìŒ ë‹¨ê³„

1. PDF ì§€ì› ì¶”ê°€
2. ë©€í‹°ëª¨ë‹¬ ì„ë² ë”© (í‘œ ì´ë¯¸ì§€)
3. ì¦ë¶„ ì—…ë°ì´íŠ¸ ì§€ì›
4. ì›¹ ì¸í„°í˜ì´ìŠ¤ êµ¬ì¶•
5. RAG ì‹œìŠ¤í…œ í†µí•©
