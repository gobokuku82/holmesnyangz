"""
ChromaDB ì¬ì„ë² ë”© ìŠ¤í¬ë¦½íŠ¸

ëª©ì : ì²­í‚¹ëœ ë²•ë ¹ íŒŒì¼ë“¤ì„ í‘œì¤€í™”ëœ ë©”íƒ€ë°ì´í„°ë¡œ ChromaDBì— ì¬ì„ë² ë”©

ì£¼ìš” ê¸°ëŠ¥:
1. ë¶ˆì¼ì¹˜í•œ ë©”íƒ€ë°ì´í„° í•„ë“œë¥¼ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ì •ê·œí™”
2. title í•„ë“œ í†µí•© (law_title/decree_title/rule_title â†’ title)
3. doc_type, category ìë™ ì¶”ì¶œ
4. ChromaDB ì¬ìƒì„± (ê¸°ì¡´ ì‚­ì œ í›„ ì¬êµ¬ì¶•)

ì‹¤í–‰ ë°©ë²•:
    python backend/data/storage/legal_info/embedding/embed_legal_documents.py --test
    python backend/data/storage/legal_info/embedding/embed_legal_documents.py --full
"""

import json
import sys
from pathlib import Path
from typing import Dict, List, Any
import chromadb
from sentence_transformers import SentenceTransformer
from datetime import datetime
import re

# ê²½ë¡œ ì„¤ì •
PROJECT_ROOT = Path(__file__).resolve().parents[5]
CHUNKED_DIR = PROJECT_ROOT / "backend" / "data" / "storage" / "legal_info" / "chunked"
CHROMA_PATH = PROJECT_ROOT / "backend" / "data" / "storage" / "legal_info" / "chroma_db"
MODEL_PATH = PROJECT_ROOT / "backend" / "models" / "kure_v1"

# ì¹´í…Œê³ ë¦¬ ë§¤í•‘
CATEGORIES = [
    "1_ê³µí†µ ë§¤ë§¤_ì„ëŒ€ì°¨",
    "2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸",
    "3_ê³µê¸‰_ë°_ê´€ë¦¬_ë§¤ë§¤_ë¶„ì–‘",
    "4_ê¸°íƒ€"
]


def extract_doc_type(filename: str) -> str:
    """
    íŒŒì¼ëª…ì—ì„œ ë¬¸ì„œ íƒ€ì… ì¶”ì¶œ

    ì˜ˆì‹œ:
        - ê³µì¸ì¤‘ê°œì‚¬ë²•(ë²•ë¥ )(ì œ19841í˜¸).json â†’ ë²•ë¥ 
        - ë¶€ë™ì‚°ê±°ë˜ì‹ ê³ ë²• ì‹œí–‰ë ¹(ëŒ€í†µë ¹ë ¹).json â†’ ì‹œí–‰ë ¹
        - ë¶€ë™ì‚°_ìš©ì–´_95ê°€ì§€_chunked.json â†’ ìš©ì–´ì§‘
    """
    filename_lower = filename.lower()

    if "ìš©ì–´" in filename or "glossary" in filename_lower:
        return "ìš©ì–´ì§‘"
    elif "ëŒ€ë²•ì›ê·œì¹™" in filename:
        return "ëŒ€ë²•ì›ê·œì¹™"
    elif "ì‹œí–‰ê·œì¹™" in filename:
        return "ì‹œí–‰ê·œì¹™"
    elif "ì‹œí–‰ë ¹" in filename:
        return "ì‹œí–‰ë ¹"
    elif "ë²•ë¥ " in filename or "(ë²•ë¥ )" in filename:
        return "ë²•ë¥ "
    else:
        return "ê¸°íƒ€"


def normalize_metadata(raw_metadata: Dict, category: str, source_file: str, chunk_id: str) -> Dict:
    """
    ì²­í‚¹ íŒŒì¼ì˜ ë¶ˆì¼ì¹˜í•œ ë©”íƒ€ë°ì´í„°ë¥¼ í‘œì¤€ ìŠ¤í‚¤ë§ˆë¡œ ë³€í™˜

    í‘œì¤€ ìŠ¤í‚¤ë§ˆ:
        í•„ìˆ˜: doc_type, title, number, enforcement_date, category, source_file,
              article_number, article_title, chunk_index, is_deleted
        ê¶Œì¥: chapter, chapter_title, section, abbreviation
        ì„ íƒ: is_tenant_protection, is_tax_related, is_delegation, is_penalty_related,
              other_law_references, term_name, term_category, term_number
    """
    normalized = {}

    # 1. doc_type ì¶”ì¶œ
    normalized["doc_type"] = extract_doc_type(source_file)

    # 2. title í†µí•© (law_title / decree_title / rule_title / glossary_title)
    title = (
        raw_metadata.get("law_title") or
        raw_metadata.get("decree_title") or
        raw_metadata.get("rule_title") or
        raw_metadata.get("glossary_title") or
        "Unknown"
    )
    # ì•½ì¹­ ì œê±° (ì˜ˆ: "ë¶€ë™ì‚° ê±°ë˜ì‹ ê³  ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹ ( ì•½ì¹­: ë¶€ë™ì‚°ê±°ë˜ì‹ ê³ ë²• ì‹œí–‰ë ¹" â†’ "ë¶€ë™ì‚° ê±°ë˜ì‹ ê³  ë“±ì— ê´€í•œ ë²•ë¥  ì‹œí–‰ë ¹")
    if " ( ì•½ì¹­:" in title:
        title = title.split(" ( ì•½ì¹­:")[0].strip()
    normalized["title"] = title

    # 3. number í†µí•©
    normalized["number"] = (
        raw_metadata.get("law_number") or
        raw_metadata.get("decree_number") or
        raw_metadata.get("rule_number") or
        ""
    )

    # 4. í•„ìˆ˜ í•„ë“œ
    normalized["enforcement_date"] = raw_metadata.get("enforcement_date", "")
    normalized["category"] = category
    normalized["source_file"] = source_file
    normalized["article_number"] = raw_metadata.get("article_number", "")
    normalized["article_title"] = raw_metadata.get("article_title", "")

    # chunk_indexëŠ” ë‚˜ì¤‘ì— ì¼ê´„ ì²˜ë¦¬ (ë™ì¼ article_number ê·¸ë£¹ ë‚´ ìˆœë²ˆ)
    normalized["chunk_index"] = 0
    normalized["is_deleted"] = raw_metadata.get("is_deleted", False)

    # 5. ê¶Œì¥ í•„ë“œ (ìˆìœ¼ë©´ í¬í•¨)
    if "chapter" in raw_metadata:
        normalized["chapter"] = raw_metadata["chapter"]
    if "chapter_title" in raw_metadata:
        normalized["chapter_title"] = raw_metadata["chapter_title"]
    if "section" in raw_metadata:
        normalized["section"] = raw_metadata["section"]
    if "abbreviation" in raw_metadata and raw_metadata["abbreviation"]:
        normalized["abbreviation"] = raw_metadata["abbreviation"]

    # 6. Boolean ì„ íƒ í•„ë“œ (trueì¼ ë•Œë§Œ í¬í•¨)
    if raw_metadata.get("is_tenant_protection"):
        normalized["is_tenant_protection"] = True
    if raw_metadata.get("is_tax_related"):
        normalized["is_tax_related"] = True
    if raw_metadata.get("is_delegation"):
        normalized["is_delegation"] = True
    if raw_metadata.get("is_penalty_related"):
        normalized["is_penalty_related"] = True

    # 7. ì°¸ì¡° ê´€ê³„
    if "other_law_references" in raw_metadata and raw_metadata["other_law_references"]:
        normalized["other_law_references"] = json.dumps(raw_metadata["other_law_references"], ensure_ascii=False)

    # 8. ìš©ì–´ì§‘ ì „ìš©
    if "term_name" in raw_metadata:
        normalized["term_name"] = raw_metadata["term_name"]
        normalized["term_category"] = raw_metadata.get("term_category", "")
        normalized["term_number"] = raw_metadata.get("term_number", 0)

    return normalized


def assign_chunk_indices(documents: List[Dict]) -> List[Dict]:
    """
    ë™ì¼í•œ title + article_numberë¥¼ ê°€ì§„ ë¬¸ì„œë“¤ì—ê²Œ chunk_index ë¶€ì—¬

    ì˜ˆì‹œ:
        ì£¼íƒì„ëŒ€ì°¨ë³´í˜¸ë²• ì œ3ì¡° â†’ chunk_index: 0, 1, 2, ...
    """
    # title + article_numberë¡œ ê·¸ë£¹í™”
    groups = {}
    for doc in documents:
        key = f"{doc['metadata']['title']}||{doc['metadata']['article_number']}"
        if key not in groups:
            groups[key] = []
        groups[key].append(doc)

    # ê° ê·¸ë£¹ ë‚´ì—ì„œ chunk_index ë¶€ì—¬
    for key, group in groups.items():
        for idx, doc in enumerate(group):
            doc['metadata']['chunk_index'] = idx

    return documents


async def embed_documents(test_mode: bool = False, category_filter: str = None):
    """
    ì²­í‚¹ íŒŒì¼ë“¤ì„ ChromaDBì— ì„ë² ë”©

    Args:
        test_mode: Trueë©´ 1ê°œ ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬ (2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸)
        category_filter: íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ ì²˜ë¦¬ (ì˜ˆ: "2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸")
    """
    start_time = datetime.now()
    print(f"\n{'='*60}")
    print(f"ChromaDB ì¬ì„ë² ë”© ì‹œì‘: {start_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    # 1. ChromaDB ì´ˆê¸°í™”
    print("1ï¸âƒ£ ChromaDB ì´ˆê¸°í™” ì¤‘...")
    chroma_client = chromadb.PersistentClient(path=str(CHROMA_PATH))

    try:
        chroma_client.delete_collection("korean_legal_documents")
        print("   âœ… ê¸°ì¡´ ì»¬ë ‰ì…˜ ì‚­ì œ ì™„ë£Œ")
    except Exception as e:
        print(f"   â„¹ï¸ ê¸°ì¡´ ì»¬ë ‰ì…˜ ì—†ìŒ: {e}")

    collection = chroma_client.create_collection(
        name="korean_legal_documents",
        metadata={"hnsw:space": "cosine"}
    )
    print("   âœ… ìƒˆ ì»¬ë ‰ì…˜ ìƒì„± ì™„ë£Œ\n")

    # 2. ì„ë² ë”© ëª¨ë¸ ë¡œë“œ
    print("2ï¸âƒ£ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...")
    model = SentenceTransformer(str(MODEL_PATH))
    print("   âœ… kure_v1 ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\n")

    # 3. ì¹´í…Œê³ ë¦¬ ê²°ì •
    if test_mode:
        categories_to_process = ["2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸"]
        print("   ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œ: 2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸ë§Œ ì²˜ë¦¬\n")
    elif category_filter:
        categories_to_process = [category_filter]
        print(f"   ğŸ¯ í•„í„° ëª¨ë“œ: {category_filter}ë§Œ ì²˜ë¦¬\n")
    else:
        categories_to_process = CATEGORIES
        print("   ğŸš€ ì „ì²´ ëª¨ë“œ: ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬\n")

    # 4. ì¹´í…Œê³ ë¦¬ë³„ ì²˜ë¦¬
    total_embedded = 0
    total_files = 0
    category_stats = {}

    for category in categories_to_process:
        category_path = CHUNKED_DIR / category

        if not category_path.exists():
            print(f"   âš ï¸ ì¹´í…Œê³ ë¦¬ í´ë” ì—†ìŒ: {category}")
            continue

        print(f"3ï¸âƒ£ ì²˜ë¦¬ ì¤‘: {category}")
        print(f"   ê²½ë¡œ: {category_path}\n")

        json_files = list(category_path.glob("*_chunked.json"))
        category_embedded = 0

        for json_file in json_files:
            try:
                with open(json_file, "r", encoding="utf-8") as f:
                    chunks = json.load(f)

                if not chunks:
                    print(f"   âš ï¸ ë¹ˆ íŒŒì¼: {json_file.name}")
                    continue

                # ë©”íƒ€ë°ì´í„° ì •ê·œí™”
                documents = []
                for chunk in chunks:
                    normalized_meta = normalize_metadata(
                        raw_metadata=chunk.get("metadata", {}),
                        category=category,
                        source_file=json_file.name,
                        chunk_id=chunk["id"]
                    )

                    documents.append({
                        "id": chunk["id"],
                        "text": chunk["text"],
                        "metadata": normalized_meta
                    })

                # chunk_index ë¶€ì—¬
                documents = assign_chunk_indices(documents)

                # ë°°ì¹˜ ì²˜ë¦¬ (100ê°œì”©)
                batch_size = 100
                for i in range(0, len(documents), batch_size):
                    batch = documents[i:i+batch_size]

                    ids = [doc["id"] for doc in batch]
                    texts = [doc["text"] for doc in batch]
                    metadatas = [doc["metadata"] for doc in batch]

                    # ì„ë² ë”© ìƒì„±
                    embeddings = model.encode(texts, show_progress_bar=False).tolist()

                    # ChromaDB ì¶”ê°€
                    collection.add(
                        ids=ids,
                        documents=texts,
                        embeddings=embeddings,
                        metadatas=metadatas
                    )

                category_embedded += len(documents)
                total_embedded += len(documents)
                total_files += 1

                print(f"   âœ… {json_file.name}: {len(documents)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")

            except Exception as e:
                print(f"   âŒ {json_file.name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")

        category_stats[category] = category_embedded
        print(f"   ğŸ“Š {category} ì™„ë£Œ: {category_embedded}ê°œ ë¬¸ì„œ\n")

    # 5. ìµœì¢… í†µê³„
    end_time = datetime.now()
    duration = (end_time - start_time).total_seconds()

    print(f"\n{'='*60}")
    print(f"âœ… ChromaDB ì¬ì„ë² ë”© ì™„ë£Œ!")
    print(f"{'='*60}")
    print(f"ğŸ“Š ì²˜ë¦¬ í†µê³„:")
    print(f"   - ì²˜ë¦¬ íŒŒì¼: {total_files}ê°œ")
    print(f"   - ì„ë² ë”© ë¬¸ì„œ: {total_embedded}ê°œ")
    print(f"   - ì†Œìš” ì‹œê°„: {duration:.2f}ì´ˆ")
    print(f"\nğŸ“ˆ ì¹´í…Œê³ ë¦¬ë³„ í†µê³„:")
    for category, count in category_stats.items():
        print(f"   - {category}: {count}ê°œ")
    print(f"\nì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"{'='*60}\n")

    return total_embedded, category_stats


def verify_embedding():
    """ì„ë² ë”© ê²°ê³¼ ê²€ì¦"""
    print("\nğŸ” ì„ë² ë”© ê²°ê³¼ ê²€ì¦ ì¤‘...\n")

    chroma_client = chromadb.PersistentClient(path=str(CHROMA_PATH))
    collection = chroma_client.get_collection("korean_legal_documents")

    # ì „ì²´ ë¬¸ì„œ ê°œìˆ˜
    total_count = collection.count()
    print(f"âœ… ì „ì²´ ë¬¸ì„œ ê°œìˆ˜: {total_count}\n")

    # Unknown title ê°œìˆ˜ í™•ì¸
    try:
        unknown_results = collection.get(
            where={"title": "Unknown"},
            limit=10000
        )
        unknown_count = len(unknown_results['ids'])
        print(f"âš ï¸ Unknown title ë¬¸ì„œ: {unknown_count}ê°œ ({unknown_count/total_count*100:.1f}%)")

        if unknown_count > 0:
            print(f"   ìƒ˜í”Œ ID: {unknown_results['ids'][:5]}")
    except Exception as e:
        print(f"   Unknown ì²´í¬ ì‹¤íŒ¨: {e}")

    # doc_type ë¶„í¬
    print(f"\nğŸ“Š doc_type ë¶„í¬:")
    for doc_type in ["ë²•ë¥ ", "ì‹œí–‰ë ¹", "ì‹œí–‰ê·œì¹™", "ëŒ€ë²•ì›ê·œì¹™", "ìš©ì–´ì§‘", "ê¸°íƒ€"]:
        try:
            results = collection.get(
                where={"doc_type": doc_type},
                limit=10000
            )
            count = len(results['ids'])
            if count > 0:
                print(f"   - {doc_type}: {count}ê°œ ({count/total_count*100:.1f}%)")
        except:
            pass

    # ì¹´í…Œê³ ë¦¬ ë¶„í¬
    print(f"\nğŸ“Š ì¹´í…Œê³ ë¦¬ ë¶„í¬:")
    for category in CATEGORIES:
        try:
            results = collection.get(
                where={"category": category},
                limit=10000
            )
            count = len(results['ids'])
            if count > 0:
                print(f"   - {category}: {count}ê°œ ({count/total_count*100:.1f}%)")
        except:
            pass

    # ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° í™•ì¸
    print(f"\nğŸ“ ìƒ˜í”Œ ë©”íƒ€ë°ì´í„° (ì²« 3ê°œ ë¬¸ì„œ):")
    sample = collection.get(limit=3, include=["metadatas"])
    for i, (doc_id, metadata) in enumerate(zip(sample['ids'], sample['metadatas']), 1):
        print(f"\n   [{i}] {doc_id}")
        print(f"       title: {metadata.get('title')}")
        print(f"       doc_type: {metadata.get('doc_type')}")
        print(f"       category: {metadata.get('category')}")
        print(f"       article_number: {metadata.get('article_number')}")
        print(f"       chunk_index: {metadata.get('chunk_index')}")

    print(f"\n{'='*60}\n")


if __name__ == "__main__":
    import asyncio

    # ëª…ë ¹í–‰ ì¸ì ì²˜ë¦¬
    if len(sys.argv) > 1:
        mode = sys.argv[1]

        if mode == "--test":
            print("ğŸ§ª í…ŒìŠ¤íŠ¸ ëª¨ë“œë¡œ ì‹¤í–‰ (2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸ë§Œ ì²˜ë¦¬)")
            asyncio.run(embed_documents(test_mode=True))
            verify_embedding()
        elif mode == "--full":
            print("ğŸš€ ì „ì²´ ëª¨ë“œë¡œ ì‹¤í–‰ (ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì²˜ë¦¬)")
            asyncio.run(embed_documents(test_mode=False))
            verify_embedding()
        elif mode.startswith("--category="):
            category = mode.split("=")[1]
            print(f"ğŸ¯ ì¹´í…Œê³ ë¦¬ í•„í„° ëª¨ë“œë¡œ ì‹¤í–‰: {category}")
            asyncio.run(embed_documents(test_mode=False, category_filter=category))
            verify_embedding()
        else:
            print("âŒ ì˜ëª»ëœ ì¸ìì…ë‹ˆë‹¤.")
            print("ì‚¬ìš©ë²•:")
            print("  python embed_legal_documents.py --test")
            print("  python embed_legal_documents.py --full")
            print("  python embed_legal_documents.py --category=2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸")
    else:
        print("ì‚¬ìš©ë²•:")
        print("  python embed_legal_documents.py --test          # í…ŒìŠ¤íŠ¸ (1ê°œ ì¹´í…Œê³ ë¦¬)")
        print("  python embed_legal_documents.py --full          # ì „ì²´ ì¬ì„ë² ë”©")
        print("  python embed_legal_documents.py --category=2_ì„ëŒ€ì°¨_ì „ì„¸_ì›”ì„¸  # íŠ¹ì • ì¹´í…Œê³ ë¦¬ë§Œ")
