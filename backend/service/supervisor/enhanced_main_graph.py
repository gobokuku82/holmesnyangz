# backend/service/supervisor/enhanced_main_graph.py
"""
Enhanced Real Estate Supervisor with LLM Integration
LangGraph 0.6.7 ê¸°ë°˜ ê³ ë„í™”ëœ Supervisor
OpenAI API í†µí•© ë° êµ¬ì¡°í™”ëœ ì¶œë ¥ ì§€ì›
"""

from typing import Dict, Any, Type, Optional, Literal, List
from langgraph.graph import StateGraph, START, END
import logging
import json
import asyncio
from datetime import datetime
from pathlib import Path
import sys

# Add parent directory for imports
sys.path.append(str(Path(__file__).parent.parent))

from service.core.base_agent import BaseAgent
from service.core.states import SupervisorState, create_supervisor_initial_state
from service.core.context import AgentContext, create_agent_context
from service.core.config import Config
from service.utils.agent_registry import AgentRegistry

logger = logging.getLogger(__name__)


# ============ LLM Helper Class ============

class LLMHelper:
    """LLM í˜¸ì¶œì„ ìœ„í•œ í—¬í¼ í´ë˜ìŠ¤"""

    def __init__(self):
        """Initialize LLM Helper with config"""
        self.config = Config()
        self._client = None
        self._initialize_client()

    def _initialize_client(self):
        """Initialize LLM client based on config"""
        provider = self.config.LLM_PROVIDER

        if provider == "openai" and self.config.OPENAI_API_KEY:
            try:
                from openai import AsyncOpenAI
                self._client = AsyncOpenAI(api_key=self.config.OPENAI_API_KEY)
                logger.info("OpenAI client initialized")
            except ImportError:
                logger.error("OpenAI library not installed")
                self._client = None
        elif provider == "azure_openai" and self.config.AZURE_OPENAI_KEY:
            try:
                from openai import AsyncAzureOpenAI
                self._client = AsyncAzureOpenAI(
                    api_key=self.config.AZURE_OPENAI_KEY,
                    azure_endpoint=self.config.AZURE_OPENAI_ENDPOINT,
                    api_version="2024-02-01"
                )
                logger.info("Azure OpenAI client initialized")
            except ImportError:
                logger.error("OpenAI library not installed")
                self._client = None
        else:
            logger.warning(f"Using mock LLM (provider={provider}, has_key={bool(self.config.OPENAI_API_KEY)})")
            self._client = None

    async def complete(
        self,
        prompt: str,
        task_type: str = "general",
        system_prompt: Optional[str] = None,
        json_mode: bool = False
    ) -> str:
        """
        LLM ì™„ì„± ìš”ì²­

        Args:
            prompt: User prompt
            task_type: Task type for model selection (intent, planning, execution, evaluation)
            system_prompt: System prompt
            json_mode: Enable JSON response mode

        Returns:
            LLM response text
        """
        if self._client is None:
            # Fallback to mock response
            return self._mock_response(prompt, task_type)

        # Get model config for task type
        model_config = self.config.get_model_config(task_type)
        model_params = self.config.DEFAULT_MODEL_PARAMS.get(task_type, {})

        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})

            kwargs = {
                "model": model_config["model"],
                "messages": messages,
                "temperature": model_params.get("temperature", 0.7),
                "max_tokens": model_params.get("max_tokens", 1000),
            }

            # Enable JSON mode if requested
            if json_mode or model_params.get("json_mode", False):
                kwargs["response_format"] = {"type": "json_object"}

            response = await self._client.chat.completions.create(**kwargs)
            return response.choices[0].message.content

        except Exception as e:
            logger.error(f"LLM call failed: {e}")
            # Fallback to mock
            return self._mock_response(prompt, task_type)

    def _mock_response(self, prompt: str, task_type: str) -> str:
        """Generate mock response based on task type"""
        if task_type == "intent":
            return json.dumps({
                "intent_type": "search",
                "confidence": 0.85,
                "entities": {
                    "region": "ê°•ë‚¨êµ¬",
                    "property_type": "ì•„íŒŒíŠ¸",
                    "deal_type": "ë§¤ë§¤",
                    "size_range": {"min": 30, "max": 35}
                },
                "requires_clarification": False
            })
        elif task_type == "planning":
            return json.dumps({
                "strategy": "sequential",
                "reasoning": "ìˆœì°¨ì  ì‹¤í–‰ì´ ì í•©í•©ë‹ˆë‹¤",
                "agents": [
                    {
                        "name": "property_search",
                        "order": 1,
                        "parallel_group": 1,
                        "params": {"region": "ê°•ë‚¨êµ¬"},
                        "expected_output": "ë§¤ë¬¼ ëª©ë¡"
                    }
                ]
            })
        elif task_type == "evaluation":
            return json.dumps({
                "quality_score": 0.85,
                "completeness": 0.90,
                "needs_retry": False,
                "summary": "ë¶„ì„ ì™„ë£Œ",
                "final_answer": "ê°•ë‚¨êµ¬ 30í‰ëŒ€ ì•„íŒŒíŠ¸ ë§¤ë¬¼ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤.",
                "confidence_level": "high"
            })
        else:
            return "Mock response"


# Singleton LLM Helper
_llm_helper = None

def get_llm_helper() -> LLMHelper:
    """Get singleton LLM helper instance"""
    global _llm_helper
    if _llm_helper is None:
        _llm_helper = LLMHelper()
    return _llm_helper


# ============ Enhanced Node Functions ============

async def analyze_intent_node(state: Dict[str, Any], runtime: Optional[Any] = None) -> Dict[str, Any]:
    """
    Enhanced Step 1: LLMì„ ì‚¬ìš©í•œ ì˜ë„ ë¶„ì„
    """
    try:
        query = state.get("query", "")
        logger.info(f"[Step 1] Enhanced intent analysis starting: {query[:50]}...")

        # Get LLM helper
        llm = get_llm_helper()

        # Create prompt
        system_prompt = """ë‹¹ì‹ ì€ ë¶€ë™ì‚° ì±—ë´‡ì˜ ì˜ë„ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ì •í™•íˆ ë¶„ì„í•˜ì—¬ ì˜ë„ì™€ í•µì‹¬ ì •ë³´ë¥¼ ì¶”ì¶œí•´ì£¼ì„¸ìš”.
ì‘ë‹µì€ ë°˜ë“œì‹œ ìœ íš¨í•œ JSON í˜•ì‹ì´ì–´ì•¼ í•©ë‹ˆë‹¤."""

        user_prompt = f"""ì‚¬ìš©ì ì§ˆë¬¸: {query}

ë‹¤ìŒ JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•˜ì„¸ìš”:
{{
    "intent_type": "search|analysis|comparison|recommendation|general ì¤‘ í•˜ë‚˜",
    "confidence": 0.0-1.0 ì‚¬ì´ì˜ ì‹ ë¢°ë„,
    "entities": {{
        "region": "ì§€ì—­ëª… (ì—†ìœ¼ë©´ null)",
        "property_type": "ì•„íŒŒíŠ¸|ì˜¤í”¼ìŠ¤í…”|ë¹Œë¼|ìƒê°€ (ì—†ìœ¼ë©´ null)",
        "deal_type": "ë§¤ë§¤|ì „ì„¸|ì›”ì„¸ (ì—†ìœ¼ë©´ null)",
        "price_range": {{"min": ìˆ«ì, "max": ìˆ«ì}} (ì—†ìœ¼ë©´ null),
        "size_range": {{"min": ìˆ«ì, "max": ìˆ«ì}} (í‰ìˆ˜, ì—†ìœ¼ë©´ null),
        "keywords": ["ì¶”ì¶œëœ", "í‚¤ì›Œë“œë“¤"]
    }},
    "requires_clarification": true ë˜ëŠ” false,
    "clarification_questions": ["í•„ìš”í•œ ê²½ìš° ëª…í™•í™” ì§ˆë¬¸ë“¤"]
}}"""

        # Call LLM
        response = await llm.complete(
            prompt=user_prompt,
            task_type="intent",
            system_prompt=system_prompt,
            json_mode=True
        )

        # Parse response
        try:
            intent = json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse LLM response: {response[:100]}...")
            # Fallback to basic parsing
            intent = {
                "intent_type": "search",
                "confidence": 0.5,
                "entities": {
                    "region": None,
                    "property_type": "ì•„íŒŒíŠ¸",
                    "deal_type": "ë§¤ë§¤"
                },
                "requires_clarification": True
            }

        logger.info(f"[Step 1] Intent analyzed: type={intent.get('intent_type')}, confidence={intent.get('confidence', 0):.2f}")

        return {
            "intent": intent,
            "intent_confidence": intent.get("confidence", 0.5),
            "status": "processing",
            "execution_step": "planning"
        }

    except Exception as e:
        logger.error(f"[Step 1] Intent analysis failed: {e}")
        return {
            "status": "failed",
            "errors": [f"Intent analysis failed: {str(e)}"]
        }


async def build_plan_node(state: Dict[str, Any], runtime: Optional[Any] = None) -> Dict[str, Any]:
    """
    Enhanced Step 2: LLMì„ ì‚¬ìš©í•œ ê³„íš ìˆ˜ë¦½
    """
    try:
        intent = state.get("intent", {})
        intent_type = intent.get("intent_type", "search")
        confidence = state.get("intent_confidence", 0.5)

        logger.info(f"[Step 2] Building plan: intent_type={intent_type}, confidence={confidence:.2f}")

        # Get LLM helper
        llm = get_llm_helper()

        # Create prompt
        system_prompt = """ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë¶„ì„ ì›Œí¬í”Œë¡œìš° ì„¤ê³„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì˜ë„ ë¶„ì„ ê²°ê³¼ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ agent ì‹¤í–‰ ê³„íšì„ ìˆ˜ë¦½í•´ì£¼ì„¸ìš”.
ë³‘ë ¬ ì‹¤í–‰ì´ ê°€ëŠ¥í•œ ê²½ìš° parallel_groupì„ ë™ì¼í•˜ê²Œ ì„¤ì •í•˜ì„¸ìš”."""

        user_prompt = f"""ì˜ë„ ë¶„ì„ ê²°ê³¼:
{json.dumps(intent, ensure_ascii=False, indent=2)}

ì‚¬ìš© ê°€ëŠ¥í•œ agents:
- property_search: ë§¤ë¬¼ ê²€ìƒ‰ ë° ì¡°íšŒ
- market_analysis: ì‹œì¥ ë¶„ì„ ë° íŠ¸ë Œë“œ
- region_comparison: ì§€ì—­ ê°„ ë¹„êµ
- investment_advisor: íˆ¬ì ì¡°ì–¸ ë° ì¶”ì²œ

JSON í˜•ì‹ìœ¼ë¡œ ì‹¤í–‰ ê³„íšì„ ì‘ì„±í•˜ì„¸ìš”:
{{
    "strategy": "sequential|parallel|hybrid ì¤‘ í•˜ë‚˜",
    "reasoning": "ê³„íš ìˆ˜ë¦½ ì´ìœ ",
    "agents": [
        {{
            "name": "agent ì´ë¦„",
            "order": ì‹¤í–‰ ìˆœì„œ,
            "parallel_group": ë³‘ë ¬ ê·¸ë£¹ ë²ˆí˜¸,
            "params": {{agentì— ì „ë‹¬í•  íŒŒë¼ë¯¸í„°}},
            "depends_on": ["ì˜ì¡´ agentë“¤"],
            "expected_output": "ì˜ˆìƒ ì¶œë ¥"
        }}
    ],
    "estimated_time": ì˜ˆìƒ ì†Œìš” ì‹œê°„(ì´ˆ),
    "fallback_plan": {{
        "trigger": "ì‹¤íŒ¨ ì¡°ê±´",
        "agents": ["ëŒ€ì²´ agentë“¤"]
    }}
}}"""

        # Call LLM
        response = await llm.complete(
            prompt=user_prompt,
            task_type="planning",
            system_prompt=system_prompt,
            json_mode=True
        )

        # Parse response
        try:
            plan = json.loads(response)
        except json.JSONDecodeError:
            logger.error(f"Failed to parse plan: {response[:100]}...")
            # Fallback to simple plan
            plan = {
                "strategy": "sequential",
                "reasoning": "ê¸°ë³¸ ìˆœì°¨ ì‹¤í–‰",
                "agents": [
                    {
                        "name": "property_search",
                        "order": 1,
                        "parallel_group": 1,
                        "params": intent.get("entities", {})
                    }
                ]
            }

        # Validate agents exist
        available_agents = AgentRegistry.list_agents()
        plan["agents"] = [
            agent for agent in plan.get("agents", [])
            if agent["name"] in available_agents or Config.LLM_SETTINGS.get("fallback_to_mock", True)
        ]

        logger.info(f"[Step 2] Plan created: strategy={plan.get('strategy')}, agents={len(plan.get('agents', []))}")

        return {
            "execution_plan": plan,
            "agent_selection": [a["name"] for a in plan.get("agents", [])],
            "status": "processing",
            "execution_step": "executing"
        }

    except Exception as e:
        logger.error(f"[Step 2] Planning failed: {e}")
        return {
            "status": "failed",
            "errors": [f"Planning failed: {str(e)}"]
        }


async def execute_plan_parallel_node(state: Dict[str, Any], runtime: Optional[Any] = None) -> Dict[str, Any]:
    """
    Enhanced Step 3: ë³‘ë ¬ ì‹¤í–‰ì„ ì§€ì›í•˜ëŠ” ê³„íš ì‹¤í–‰
    """
    try:
        execution_plan = state.get("execution_plan", {})
        agents = execution_plan.get("agents", [])
        strategy = execution_plan.get("strategy", "sequential")

        logger.info(f"[Step 3] Executing plan: strategy={strategy}, agents={len(agents)}")

        # Prepare context
        context = {
            "chat_user_ref": state.get("chat_user_ref", "system"),
            "chat_session_id": state.get("chat_session_id", "default"),
            "chat_thread_id": state.get("chat_thread_id"),
            "original_query": state.get("query", ""),
            "llm_provider": Config.LLM_PROVIDER,
            "language": "ko"
        }

        agent_results = {}
        failed_agents = []

        if strategy == "parallel" or strategy == "hybrid":
            # Group agents by parallel_group
            groups = {}
            for agent_config in agents:
                group = agent_config.get("parallel_group", agent_config["order"])
                if group not in groups:
                    groups[group] = []
                groups[group].append(agent_config)

            # Execute groups in order
            for group_num in sorted(groups.keys()):
                group_agents = groups[group_num]
                logger.info(f"[Step 3] Executing parallel group {group_num}: {[a['name'] for a in group_agents]}")

                # Execute agents in parallel
                tasks = []
                for agent_config in group_agents:
                    task = execute_single_agent(agent_config, context, agent_results)
                    tasks.append(task)

                # Wait for all tasks in group
                results = await asyncio.gather(*tasks, return_exceptions=True)

                # Process results
                for agent_config, result in zip(group_agents, results):
                    agent_name = agent_config["name"]
                    if isinstance(result, Exception):
                        logger.error(f"Agent {agent_name} failed: {result}")
                        agent_results[agent_name] = {"status": "error", "error": str(result)}
                        failed_agents.append(agent_name)
                    else:
                        agent_results[agent_name] = result
                        if result.get("status") == "error":
                            failed_agents.append(agent_name)
        else:
            # Sequential execution
            for agent_config in agents:
                agent_name = agent_config["name"]
                result = await execute_single_agent(agent_config, context, agent_results)
                agent_results[agent_name] = result
                if result.get("status") == "error":
                    failed_agents.append(agent_name)

        logger.info(f"[Step 3] Execution complete: success={len(agent_results)-len(failed_agents)}, failed={len(failed_agents)}")

        # Update retry count if needed
        retry_count = state.get("retry_count", 0)
        if failed_agents and retry_count > 0:
            retry_count += 1

        return {
            "agent_results": agent_results,
            "failed_agents": failed_agents,
            "agent_metrics": {
                name: {"execution_time": 0.5, "success": name not in failed_agents}
                for name in agent_results
            },
            "retry_count": retry_count,
            "status": "processing",
            "execution_step": "evaluating"
        }

    except Exception as e:
        logger.error(f"[Step 3] Execution failed: {e}")
        return {
            "status": "failed",
            "errors": [f"Execution failed: {str(e)}"]
        }


async def execute_single_agent(
    agent_config: Dict[str, Any],
    context: Dict[str, Any],
    previous_results: Dict[str, Any]
) -> Dict[str, Any]:
    """Execute a single agent"""
    agent_name = agent_config["name"]
    params = agent_config.get("params", {})

    try:
        logger.info(f"Executing agent: {agent_name}")

        # Get agent from registry
        agent = AgentRegistry.get_agent(agent_name)

        # Add context and previous results to params
        execution_params = {
            **params,
            "context": context,
            "previous_results": previous_results
        }

        # Execute agent
        result = await agent.execute(execution_params)

        logger.info(f"Agent {agent_name} completed successfully")
        return result

    except Exception as e:
        logger.error(f"Agent {agent_name} execution failed: {e}")
        return {
            "status": "error",
            "agent": agent_name,
            "error": str(e)
        }


async def evaluate_results_node(state: Dict[str, Any], runtime: Optional[Any] = None) -> Dict[str, Any]:
    """
    Enhanced Step 4: LLMì„ ì‚¬ìš©í•œ ê²°ê³¼ í‰ê°€
    """
    try:
        agent_results = state.get("agent_results", {})
        failed_agents = state.get("failed_agents", [])
        query = state.get("query", "")
        intent = state.get("intent", {})

        logger.info(f"[Step 4] Evaluating results: {len(agent_results)} results")

        # Get LLM helper
        llm = get_llm_helper()

        # Prepare results summary
        results_summary = {
            agent_name: {
                "status": result.get("status"),
                "data_count": len(result.get("data", [])) if isinstance(result.get("data"), list) else 0,
                "has_insights": bool(result.get("insights")),
                "has_recommendations": bool(result.get("recommendations"))
            }
            for agent_name, result in agent_results.items()
        }

        # Create evaluation prompt
        system_prompt = """ë‹¹ì‹ ì€ ë¶€ë™ì‚° ë¶„ì„ ê²°ê³¼ í‰ê°€ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì‹¤í–‰ ê²°ê³¼ë¥¼ ì¢…í•©í•˜ì—¬ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ì‚¬ìš©ì ì¹œí™”ì ì¸ ë‹µë³€ì„ ìƒì„±í•´ì£¼ì„¸ìš”."""

        user_prompt = f"""ì›ë˜ ì§ˆë¬¸: {query}
ì˜ë„: {json.dumps(intent, ensure_ascii=False)}
ì‹¤í–‰ ê²°ê³¼ ìš”ì•½: {json.dumps(results_summary, ensure_ascii=False)}
ì‹¤íŒ¨í•œ agents: {failed_agents}

JSON í˜•ì‹ìœ¼ë¡œ í‰ê°€ ê²°ê³¼ë¥¼ ì‘ì„±í•˜ì„¸ìš”:
{{
    "quality_score": 0.0-1.0,
    "completeness": 0.0-1.0,
    "needs_retry": true ë˜ëŠ” false,
    "retry_agents": ["ì¬ì‹œë„ í•„ìš” agentë“¤"],
    "missing_information": ["ëˆ„ë½ëœ ì •ë³´"],
    "summary": "ê²°ê³¼ ìš”ì•½",
    "final_answer": "ì‚¬ìš©ìì—ê²Œ ì „ë‹¬í•  ë‹µë³€ (ìì—°ìŠ¤ëŸ¬ìš´ í•œêµ­ì–´)",
    "confidence_level": "high|medium|low",
    "recommendations": ["ì¶”ê°€ ì¶”ì²œì‚¬í•­"]
}}"""

        # Call LLM
        response = await llm.complete(
            prompt=user_prompt,
            task_type="evaluation",
            system_prompt=system_prompt,
            json_mode=True
        )

        # Parse response
        try:
            evaluation = json.loads(response)
        except json.JSONDecodeError:
            # Fallback evaluation
            evaluation = {
                "quality_score": 0.7,
                "completeness": 0.8,
                "needs_retry": len(failed_agents) > 0,
                "retry_agents": failed_agents,
                "summary": "ë¶„ì„ ì™„ë£Œ",
                "final_answer": _generate_basic_answer(query, agent_results),
                "confidence_level": "medium"
            }

        # Check retry logic
        retry_count = state.get("retry_count", 0)
        max_retries = state.get("max_retries", 2)
        needs_retry = evaluation.get("needs_retry", False) and retry_count < max_retries

        logger.info(f"[Step 4] Evaluation complete: quality={evaluation.get('quality_score', 0):.2f}, retry={needs_retry}")

        # Create final output
        final_output = {
            "answer": evaluation.get("final_answer", ""),
            "data": agent_results,
            "metadata": {
                "quality_score": evaluation.get("quality_score", 0),
                "completeness": evaluation.get("completeness", 0),
                "confidence_level": evaluation.get("confidence_level", "low"),
                "agents_used": list(agent_results.keys()),
                "execution_time": datetime.now().isoformat(),
                "retry_count": retry_count
            },
            "recommendations": evaluation.get("recommendations", [])
        }

        return {
            "evaluation": evaluation,
            "quality_score": evaluation.get("quality_score", 0),
            "final_output": final_output,
            "retry_needed": needs_retry,
            "retry_agents": evaluation.get("retry_agents", []) if needs_retry else [],
            "status": "completed" if not needs_retry else "processing",
            "execution_step": "finished" if not needs_retry else "retrying"
        }

    except Exception as e:
        logger.error(f"[Step 4] Evaluation failed: {e}")
        return {
            "status": "failed",
            "errors": [f"Evaluation failed: {str(e)}"]
        }


def _generate_basic_answer(query: str, agent_results: Dict[str, Any]) -> str:
    """Generate basic answer without LLM"""
    parts = [f"'{query}'ì— ëŒ€í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.\n"]

    for agent_name, result in agent_results.items():
        if result.get("status") == "success":
            if "data" in result and result["data"]:
                parts.append(f"â€¢ {agent_name}: {len(result['data'])}ê°œ ê²°ê³¼")
            if "insights" in result:
                parts.append(f"â€¢ ë¶„ì„: {result['insights'][0] if result['insights'] else 'ì™„ë£Œ'}")

    if not parts[1:]:
        return "ì£„ì†¡í•©ë‹ˆë‹¤. ìš”ì²­í•˜ì‹  ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

    return "\n".join(parts)


# ============ Enhanced Supervisor Class ============

class EnhancedRealEstateSupervisor(BaseAgent):
    """
    ê³ ë„í™”ëœ ë¶€ë™ì‚° ì±—ë´‡ Supervisor
    - LLM í†µí•© (OpenAI/Azure/Mock)
    - ë³‘ë ¬ ì‹¤í–‰ ì§€ì›
    - êµ¬ì¡°í™”ëœ ì¶œë ¥
    """

    def __init__(
        self,
        agent_name: str = "enhanced_supervisor",
        checkpoint_dir: Optional[str] = None,
        max_retries: int = 2
    ):
        """Initialize enhanced supervisor"""
        self.max_retries = max_retries
        super().__init__(agent_name, checkpoint_dir)

        # Initialize LLM helper
        self.llm_helper = get_llm_helper()

        logger.info(f"EnhancedRealEstateSupervisor initialized (provider={Config.LLM_PROVIDER})")

    def _get_state_schema(self) -> Type:
        """State schema"""
        return SupervisorState

    def _build_graph(self) -> None:
        """
        Build enhanced workflow graph
        """
        self.workflow = StateGraph(
            state_schema=SupervisorState,
            context_schema=AgentContext
        )

        # Add nodes
        self.workflow.add_node("analyze_intent", analyze_intent_node)
        self.workflow.add_node("build_plan", build_plan_node)
        self.workflow.add_node("execute_plan", execute_plan_parallel_node)
        self.workflow.add_node("evaluate_results", evaluate_results_node)

        # Add edges
        self.workflow.add_edge(START, "analyze_intent")
        self.workflow.add_edge("analyze_intent", "build_plan")
        self.workflow.add_edge("build_plan", "execute_plan")
        self.workflow.add_edge("execute_plan", "evaluate_results")

        # Conditional edges
        self.workflow.add_conditional_edges(
            "evaluate_results",
            self._should_retry,
            {
                "retry": "execute_plan",
                "end": END
            }
        )

        logger.info("Enhanced workflow graph built")

    def _should_retry(self, state: Dict[str, Any]) -> Literal["retry", "end"]:
        """Determine if retry is needed"""
        needs_retry = state.get("retry_needed", False)
        retry_count = state.get("retry_count", 0)

        if needs_retry and retry_count < self.max_retries:
            logger.info(f"Retrying (attempt {retry_count + 1}/{self.max_retries})")
            return "retry"
        return "end"

    async def _validate_input(self, input_data: Dict[str, Any]) -> bool:
        """Validate input"""
        if "query" not in input_data:
            logger.error("Missing 'query' field")
            return False

        query = input_data["query"]
        if not isinstance(query, str) or len(query.strip()) == 0:
            logger.error("Invalid query")
            return False

        return True

    def _create_initial_state(self, input_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create initial state with LLM config"""
        state = create_supervisor_initial_state(
            chat_session_id=input_data.get("chat_session_id", f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"),
            query=input_data.get("query", ""),
            max_retries=self.max_retries
        )

        # Add LLM configuration to state
        state["llm_provider"] = Config.LLM_PROVIDER
        state["llm_model"] = Config.DEFAULT_MODELS.get("general")

        return state

    async def ask(self, query: str, session_id: str = None) -> Dict[str, Any]:
        """
        Simple interface for asking questions

        Args:
            query: User question
            session_id: Optional session ID

        Returns:
            Final response with answer and metadata
        """
        input_data = {
            "query": query,
            "chat_session_id": session_id or f"session_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
        }

        result = await self.execute(input_data)

        if result["status"] == "success":
            return result["data"].get("final_output", {})
        else:
            return {
                "error": result.get("error", "Unknown error"),
                "status": "failed"
            }


# ============ Testing ============

async def test_enhanced_supervisor():
    """Test enhanced supervisor"""

    # Setup logging
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    )

    supervisor = EnhancedRealEstateSupervisor()

    test_queries = [
        "ê°•ë‚¨ì—­ ê·¼ì²˜ 30í‰ëŒ€ ì•„íŒŒíŠ¸ ë§¤ë§¤ ì‹œì„¸ ì•Œë ¤ì¤˜",
        "ì„œì´ˆêµ¬ì™€ ê°•ë‚¨êµ¬ ì•„íŒŒíŠ¸ ê°€ê²© ë¹„êµ ë¶„ì„í•´ì¤˜",
        "íˆ¬ìí•˜ê¸° ì¢‹ì€ ì§€ì—­ ì¶”ì²œí•´ì¤˜",
        "ì†¡íŒŒêµ¬ ì ì‹¤ë™ 40í‰ ì•„íŒŒíŠ¸ ì „ì„¸ ì‹œì„¸ëŠ”?"
    ]

    for query in test_queries:
        print(f"\n{'='*70}")
        print(f"ğŸ“ ì§ˆë¬¸: {query}")
        print(f"{'='*70}")

        result = await supervisor.ask(query)

        if "error" in result:
            print(f"âŒ ì˜¤ë¥˜: {result['error']}")
        else:
            print(f"\nâœ… ë‹µë³€:")
            print(result.get('answer', 'No answer'))

            if "metadata" in result:
                meta = result["metadata"]
                print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„°:")
                print(f"  â€¢ í’ˆì§ˆ ì ìˆ˜: {meta.get('quality_score', 0):.2f}")
                print(f"  â€¢ ì™„ì„±ë„: {meta.get('completeness', 0):.2f}")
                print(f"  â€¢ ì‹ ë¢°ë„: {meta.get('confidence_level', 'unknown')}")
                print(f"  â€¢ ì‚¬ìš©ëœ agents: {', '.join(meta.get('agents_used', []))}")
                print(f"  â€¢ ì¬ì‹œë„ íšŸìˆ˜: {meta.get('retry_count', 0)}")

            if "recommendations" in result and result["recommendations"]:
                print(f"\nğŸ’¡ ì¶”ì²œì‚¬í•­:")
                for rec in result["recommendations"]:
                    print(f"  â€¢ {rec}")


if __name__ == "__main__":
    asyncio.run(test_enhanced_supervisor())